{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcc0b97",
   "metadata": {},
   "source": [
    "# tex to pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6eadf",
   "metadata": {},
   "source": [
    "given the source directory copies the modified sty file to all valid folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36219fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tex2pdf import latex_sources_to_pdf\n",
    "source_directory=\"/Users/mv96/new/tkb-srcs\"\n",
    "new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\"\n",
    "\n",
    "res=latex_sources_to_pdf(source_directory=source_directory,new_extthm_path=new_extthm_path,n_jobs=4).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "neg=[]\n",
    "\n",
    "for element in res:\n",
    "    if(element[0]==0):\n",
    "        pos.append(element[1])\n",
    "    else:\n",
    "        neg.append(element[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pos),len(neg))\n",
    "#4899 820 processed pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7844b",
   "metadata": {},
   "source": [
    "# Grobid preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_clean import Preprocess_using_grobid\n",
    "import time\n",
    "tick=time.time()\n",
    "prep=Preprocess_using_grobid()\n",
    "final=prep.fit(grobid_xml=\"/Users/mv96/new/dest-pdfs/1911.02675/main.tei.xml\",show_results=True)\n",
    "tock=time.time()\n",
    "print(tock-tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6ea2e",
   "metadata": {},
   "source": [
    "# visualizing boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9891501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from visualize_annot import annotations_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5597720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try this - /Users/mv96/Desktop/temp/1902.11202/1902.11202_annot.xml\n",
    "#/Users/mv96/Desktop/temp/1910.12458/1910.12458_annot.xml\n",
    "annot=annotations_page(labels_xml=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202_annot.xml\",show_images=True)\n",
    "\n",
    "annotations=annot.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5469397",
   "metadata": {},
   "source": [
    "# assigning color labels to the fonts (using latex sources) for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try=/Users/mv96/new/dest-pdfs/1509.06361/file.tei.xml\n",
    "from labelling import assigning_labels\n",
    "\n",
    "#name 1712.06239/main.pdf\n",
    "grobid_xml=\"/Users/mv96/new/test/1412.5657/mono-submit-full.tei.xml\" #grobid xml\n",
    "labels_xml=\"/Users/mv96/new/test/1412.5657/mono-submit-full_annot.xml\" #annotations xml\n",
    "\n",
    "table,scales=assigning_labels(show_images=True, \n",
    "              grobid_xml=grobid_xml,\n",
    "              labels_xml=labels_xml).fit()\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda89f6",
   "metadata": {},
   "source": [
    "# vectorizing fonts from a pdfalto xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonts_vector import fonts2vec\n",
    "\n",
    "excel_file=\"output.xlsx\" #the path xlsx of the manually labelled fonts\n",
    "pdf_alto_xml_main=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202.xml\" #pdfalto xml\n",
    "\n",
    "#takes xml file containing all the fonts and the manual labelling file\n",
    "vectorized_fonts=fonts2vec(pdf_alto_xml_main,excel_file).get_dataframe()\n",
    "vectorized_fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5cebc",
   "metadata": {},
   "source": [
    "# merging the outputs from pdf alto and grobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd28619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataframe_for_eval import preprocessed_dataframe\n",
    "\n",
    "grobid_xml=\"/Users/mv96/new/dest-pdfs/1408.5412/role.tei.xml\" #grobid xml\n",
    "labels_xml=\"/Users/mv96/new/dest-pdfs/1408.5412/role_annot.xml\" #annotations xml\n",
    "excel_file=\"output.xlsx\" #the path xlsx of the manually labelled fonts\n",
    "pdf_alto_xml_main=\"/Users/mv96/new/dest-pdfs/1408.5412/role.xml\" #pdfalto xml\n",
    "sample_pdf=\"/Users/mv96/new/dest-pdfs/1408.5412/role.pdf\" #sample_pdf\n",
    "\n",
    "prep=preprocessed_dataframe(grobid_xml=grobid_xml,labels_xml=labels_xml,excel_file=excel_file,\n",
    "                       pdf_alto_xml_main=pdf_alto_xml_main,\n",
    "                      sample_pdf=sample_pdf,save_state=True)\n",
    "\n",
    "prep.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90dab8a",
   "metadata": {},
   "source": [
    "# putting every thing together (for generating pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#1\n",
    "from tex2pdf import latex_sources_to_pdf\n",
    "res=latex_sources_to_pdf(source_directory=\"/Users/mv96/new/tkb-srcs\",n_jobs=4,new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\").fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]; neg=[];\n",
    "for element in res:\n",
    "    if(element[0]==1):\n",
    "        neg.append(element[1])\n",
    "    else:\n",
    "        pos.append(element[1])\n",
    "        \n",
    "        \n",
    "print(len(pos),len(neg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c065940",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76034664",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "errors_dict={}\n",
    "for element in res:\n",
    "    if(len(element)==2):\n",
    "        pos.append(element[1])\n",
    "    else:\n",
    "        error=str(element[2])\n",
    "        error_file=element[1]\n",
    "        skip=True\n",
    "        for key in errors_dict.keys():\n",
    "            if(key[:10]==error[:10]):\n",
    "                errors_dict[key].append(error_file)\n",
    "                skip=False\n",
    "                break\n",
    "            \n",
    "        if(skip):\n",
    "            errors_dict[error]=[error_file]\n",
    "\n",
    "print(len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807de6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 build pdfs from latex sources \n",
    "#2 move the valid pdf's from the source to a new directory\n",
    "#3 initialize the grobid server and generate the tei file \n",
    "#4 get the outputs from the pdfalto annot_xml and .xml\n",
    "#5 filter / delete invalid directories to confine the dataset\n",
    "#6 apply the preprocessing in batch\n",
    "\n",
    "################################################\n",
    "source_directory=\"/Users/mv96/new/tkb-srcs\"\n",
    "n_jobs=4\n",
    "new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\"\n",
    "destination=\"/Users/mv96/new/dest-pdfs\"\n",
    "grobid_directory=\"/Users/mv96/grobid\"\n",
    "grobid_client=\"/Users/mv96/grobid_client_python\"\n",
    "pdfalto=\"/Users/mv96/Downloads/pdfalto/pdfalto\"\n",
    "#####################################################################\n",
    "\n",
    "import os\n",
    "\n",
    "#1\n",
    "from tex2pdf import latex_sources_to_pdf\n",
    "res=latex_sources_to_pdf(source_directory=source_directory,n_jobs=n_jobs,new_extthm_path=new_extthm_path).fit()\n",
    "\n",
    "\n",
    "###############error control ##################################\n",
    "es=[]\n",
    "for element in res:\n",
    "    if(element[0]!=0):\n",
    "        es.append(element)\n",
    "        \n",
    "s={}        \n",
    "for element in es:\n",
    "    if(type(element[-1]) not in s):\n",
    "        string_name=str(type(element[-1]).__name__)\n",
    "        if(string_name not in s):\n",
    "            s[string_name]=[element[1]]\n",
    "        else:\n",
    "            s[string_name].append(element[1])\n",
    "s\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8b813",
   "metadata": {},
   "source": [
    "# Implementations (for package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying sty \n",
    "#compiling pdfs\n",
    "#and saving it to a new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e843fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "source_directory=\"/Users/mv96/new/tkb-srcs\"\n",
    "n_jobs=-2\n",
    "new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\"\n",
    "destination=\"/Users/mv96/new/dest-pdfs\"\n",
    "grobid_directory=\"/Users/mv96/grobid\"\n",
    "grobid_client=\"/Users/mv96/grobid_client_python\"\n",
    "pdfalto=\"/Users/mv96/Downloads/pdfalto/pdfalto\"\n",
    "\n",
    "\n",
    "from tex2pdf import latex_sources_to_pdf\n",
    "temp=\"/Users/mv96/new/tkb-srcs/1507.03439/weightreduction.tex\"\n",
    "c=latex_sources_to_pdf(source_directory=source_directory,n_jobs=n_jobs,new_extthm_path=new_extthm_path)\n",
    "\n",
    "#try on one pdf\n",
    "print(c.generate_pdf(temp))\n",
    "\n",
    "#but try on all\n",
    "compiled_pdfs=latex_sources_to_pdf(source_directory=source_directory,n_jobs=n_jobs,new_extthm_path=new_extthm_path).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many pdfs made pass through the first step\n",
    "counts=list(map(lambda x: x[0],compiled_pdfs))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b89e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 copy these pdf files to another destination\n",
    "\n",
    "from copy_dest import copy_dest\n",
    "copy_dest(source=source_directory, destination=destination).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35faeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of errors faced so far\n",
    "\n",
    "es=[]\n",
    "for element in compiled_pdfs:\n",
    "    if(element[0]!=0):\n",
    "        es.append(element)\n",
    "        \n",
    "s={}        \n",
    "for element in compiled_pdfs:\n",
    "    if(type(element[-1]) not in s):\n",
    "        string_name=str(type(element[-1]).__name__)\n",
    "        if(string_name not in s):\n",
    "            s[string_name]=[element[1]]\n",
    "        else:\n",
    "            s[string_name].append(element[1])\n",
    "\n",
    "s.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68737e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#might need to run it twice if throws an error\n",
    "#3\n",
    "import os\n",
    "sub_folders=[os.path.join(destination,element) for element in os.listdir(destination)] #a list grobid_valid_paths\n",
    "\n",
    "from grobid_on_pdfs import grobid_on_pdfs\n",
    "grobid_on_pdfs(grobid_client=grobid_client,grobid_directory=grobid_directory,sub_folders=sub_folders,n_jobs=4).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying pdfalto on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source_directory=\"/Users/mv96/new/tkb-srcs\"\n",
    "n_jobs=4\n",
    "new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\"\n",
    "destination=\"/Users/mv96/tkb_pdfs\"\n",
    "grobid_directory=\"/Users/mv96/grobid\"\n",
    "grobid_client=\"/Users/mv96/grobid_client_python\"\n",
    "pdfalto=\"/Users/mv96/Downloads/pdfalto/pdfalto\"\n",
    "#4\n",
    "from pdfalto_on_pdfs import pdfalto_on_pdfs\n",
    "sub_folders=os.listdir(destination)\n",
    "pdfs=pdfalto_on_pdfs(sub_folders=sub_folders,pdfalto=pdfalto,n_jobs=4,main_folder=destination).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa028f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory=\"/Users/mv96/new/tkb-srcs\"\n",
    "n_jobs=4\n",
    "new_extthm_path=\"/Users/mv96/Desktop/temp/1910.12458/extthm.sty\"\n",
    "destination=\"/Users/mv96/new/dest-pdfs\"\n",
    "grobid_directory=\"/Users/mv96/grobid\"\n",
    "grobid_client=\"/Users/mv96/grobid_client_python\"\n",
    "pdfalto=\"/Users/mv96/Downloads/pdfalto/pdfalto\"\n",
    "\n",
    "\n",
    "#5 filtering the outputs generated using the annot_file\n",
    "from pdf_is_valid import pdf_is_valid\n",
    "pos,neg=pdf_is_valid(destination,n_jobs=4).fit()\n",
    "\n",
    "print(len(pos),len(neg))\n",
    "#2665 858\n",
    "\n",
    "with open('negative_pdfs.txt', 'w') as f:\n",
    "    for sample in neg:\n",
    "        f.write(sample+\"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32301b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=\"/Users/mv96/new/dest-pdfs\"\n",
    "sub_folders=os.listdir(path)\n",
    "pdfs=[]\n",
    "for sub in sub_folders:\n",
    "    try:\n",
    "        new=list(filter(lambda x: x.endswith(\".pdf\"),os.listdir(os.path.join(path,sub))))\n",
    "        print(os.path.join(sub_folders,new))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_on_pdfs import grobid_on_pdfs\n",
    "grobid_on_pdfs(grobid_client=grobid_client,grobid_directory=grobid_directory,sub_folders=sub_folders,n_jobs=4).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592778a5",
   "metadata": {},
   "source": [
    "# gunzipping from source directory \n",
    "\n",
    "## extract all gzip from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gunzip to \n",
    "from sh import gunzip\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unzip_file(file_location):\n",
    "    try:\n",
    "        gunzip(file_location)\n",
    "    except:\n",
    "        return file_location\n",
    "\n",
    "\n",
    "def gunzip_from_drive(gunzip_dir,n_jobs=4):\n",
    "    \"\"\"returns the gzip file locations\"\"\"\n",
    "    gz_paths=gunzip_dir\n",
    "    all_files=[]\n",
    "    for path in os.listdir(gz_paths):\n",
    "        sub_zip_path=os.path.join(gz_paths,path)\n",
    "        files=os.listdir(sub_zip_path)\n",
    "        files=list(map(lambda x: os.path.join(sub_zip_path,x) , files))\n",
    "        all_files+=files\n",
    "    \n",
    "    gunzip_files=list(filter(lambda x: x.endswith(\".gz\"),all_files))\n",
    "    unsuccessful_files=Parallel(n_jobs=n_jobs)(delayed(unzip_file)(gz) for gz in tqdm(gunzip_files))\n",
    "    return unsuccessful_files\n",
    "    \n",
    "\n",
    "#this function wont return anything but it will unzip the files in the destination directory\n",
    "unsuccessful_zips=gunzip_from_drive(gunzip_dir=\"/Volumes/MV96 /aws_data_pos/src-pos\",n_jobs=4)\n",
    "print(len(unsuccessful_zips))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gzs=list(filter(lambda x: x.endswith(\".gz\"),os.listdir(gz_01_path)))\n",
    "print(len(gzs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18c143",
   "metadata": {},
   "source": [
    "# few plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/mv96/Desktop/temp/post_doc_eng/package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing co relation plots\n",
    "\n",
    "new=temp.iloc[:,6:]\n",
    "\n",
    "def map_labels(val):\n",
    "    val=val.split(\":\")\n",
    "    if(len(val)==1):\n",
    "        return val[0]\n",
    "    else:\n",
    "        try:\n",
    "            return val[1].split(\".\")[-3]\n",
    "        except:\n",
    "            return val[1].split(\".\")[-2]\n",
    "        \n",
    "\n",
    "new=new[new[\"label\"]!=\"overlap\"]\n",
    "new[\"label\"]=new[\"label\"].apply(map_labels)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81553035",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca74f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"label\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as mp\n",
    "mask = np.triu(np.ones_like(new.corr(method=\"spearman\")))\n",
    " \n",
    "# plotting a triangle correlation heatmap\n",
    "dataplot = sns.heatmap(new.corr(method=\"spearman\"), mask=mask)\n",
    " \n",
    "# displaying heatmap\n",
    "mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf387d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(new, x=\"is_italic_manual\", hue=\"label\", kind=\"kde\", fill=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f8c10",
   "metadata": {},
   "source": [
    "# getting the fonts vector with merged labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134c045",
   "metadata": {},
   "source": [
    "1. applies preprocessing given grobid pdfalto and true annotations along with pdf\n",
    "2. scale the preprocessing step to all pdf either sequentially or in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataframe_for_eval import preprocessed_dataframe\n",
    "\n",
    "grobid_xml=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202.tei.xml\" #grobid xml\n",
    "labels_xml=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202_annot.xml\" #annotations xml\n",
    "excel_file=\"output.xlsx\" #the path xlsx of the manually labelled fonts\n",
    "pdf_alto_xml_main=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202.xml\" #pdfalto xml\n",
    "sample_pdf=\"/Users/mv96/Desktop/temp/1902.11202/1902.11202.pdf\" #sample_pdf\n",
    "\n",
    "prep=preprocessed_dataframe(grobid_xml=grobid_xml,labels_xml=labels_xml,excel_file=excel_file,\n",
    "                       pdf_alto_xml_main=pdf_alto_xml_main,\n",
    "                      sample_pdf=sample_pdf,save_state=True)\n",
    "prep.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f42492",
   "metadata": {},
   "source": [
    "# generate csv files from the annot xml and tei.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generates a list of valid folders\n",
    "n_jobs=4\n",
    "destination=\"/Users/mv96/new/dest-pdfs\"\n",
    "\n",
    "#5 filtering the outputs generated using the annot_file\n",
    "from pdf_is_valid import pdf_is_valid\n",
    "pos,neg=pdf_is_valid(destination,n_jobs=4).fit()\n",
    "\n",
    "excel_file=\"output.xlsx\" #the path xlsx of the manually labelled fonts\n",
    "\n",
    "\n",
    "def generate_input_files(name,excel_file):\n",
    "    grobid_xml=name.replace(\".pdf\",\".tei.xml\")\n",
    "    labels_xml=name.replace(\".pdf\",\"_annot.xml\") #annotations xml\n",
    "    pdf_alto_xml_main=name.replace(\".pdf\",\".xml\")\n",
    "    sample_pdf=name\n",
    "    return [grobid_xml,labels_xml,excel_file,pdf_alto_xml_main,sample_pdf]\n",
    "\n",
    "files_for_df=[generate_input_files(file,excel_file) for file in pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9169e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from dataframe_for_eval import preprocessed_dataframe\n",
    "import os\n",
    "\n",
    "for file,element in tqdm(zip(files_for_df,prep)):\n",
    "    try:\n",
    "        if(os.path.exists(file[-1].rsplit(\"/\",1)[0]+\"/data.csv\")):   \n",
    "            continue\n",
    "        else:\n",
    "            print(\"=\"*20)\n",
    "            print(file[-1])\n",
    "            element.fit()\n",
    "            print(\"done\")\n",
    "    except Exception as exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a3504",
   "metadata": {},
   "source": [
    "# post preprocessing step renders a csv file in the corresponding directory \n",
    "\n",
    "1. collect all the directory/ path of csv where the data.csv files are present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for all the csv files in the directory make a lis tof them \n",
    "#check if the csv file has more than or equal to 2 tyes of annotations other than basic and overlap {like proofs and theorems}\n",
    "#we can use the data from the csv \n",
    "#else we will partition the rows that containt the annotations basically not including the annot from the \n",
    "successfuls=[]\n",
    "for sub_folder in list(os.listdir(destination)):\n",
    "    try:\n",
    "        file_list=list(os.listdir(os.path.join(destination,sub_folder)))\n",
    "    except:\n",
    "        continue\n",
    "    #filter the files that end with .csv\n",
    "    dot_csv_files=list(filter(lambda x: x.endswith(\".csv\"), file_list))\n",
    "    if(len(dot_csv_files)==1):\n",
    "        full_path = os.path.join(destination,sub_folder,dot_csv_files[0])\n",
    "        successfuls.append(full_path)\n",
    "\n",
    "print(len(successfuls))\n",
    "\n",
    "#successfuls contains all the pdfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(successfuls)\n",
    "\n",
    "name=\"/Users/mv96/new/dest-pdfs/1812.02037/data.csv\"\n",
    "\n",
    "import os\n",
    "\n",
    "def map_csv_to_xml(name):\n",
    "    t=name.rsplit(\"/\",1)\n",
    "    file_name=list(filter(lambda x: x.endswith(\".tei.xml\") ,os.listdir(t[0])))\n",
    "    file_name=os.path.join(t[0],file_name[0])\n",
    "    if(os.path.exists(file_name)):\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "map_csv_to_xml(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corresponding_xmls=list(map(lambda x: map_csv_to_xml(x), successfuls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_headers:\n",
    "    \n",
    "    def __init__(self,directory_file,n_jobs=1):\n",
    "        \n",
    "        #necessary imports\n",
    "        import os\n",
    "        from tqdm import tqdm\n",
    "        from joblib import Parallel, delayed\n",
    "        self.n_jobs=n_jobs\n",
    "        \n",
    "        if(isinstance(directory_file,list)):\n",
    "            self.mode=\"list_batch\"\n",
    "            self.files_to_run=directory_file\n",
    "        elif(\".xml\" in directory_file):\n",
    "            self.mode=\"file\"\n",
    "            self.file_to_run=directory_file\n",
    "        else:\n",
    "            self.mode=\"folder\"\n",
    "            \n",
    "    def read_file_xml_to_bs4(self,xml_file):\n",
    "        \"\"\"reads xml returns bs4 object\"\"\"\n",
    "        with open(xml_file, \"r\") as file:\n",
    "            # Read each line in the file, readlines() returns a list of lines\n",
    "            content = file.readlines()\n",
    "            # Combine the lines in the list into a string\n",
    "            content = \"\".join(content)\n",
    "            bs_content = bs(content, \"xml\")\n",
    "            file.close()\n",
    "        return bs_content\n",
    "    \n",
    "    def extract_header_info(self,bs_object,show_results=False):\n",
    "        #get the header information\n",
    "        try:\n",
    "            header_info=bs_object.teiHeader\n",
    "        except:\n",
    "            header=\"\"\n",
    "        #what can we get inside the header\n",
    "\n",
    "        ##title\n",
    "        try:\n",
    "            title=header_info.find(\"title\").get_text()\n",
    "        except:\n",
    "            title=\"\"\n",
    "\n",
    "        ##publisher\n",
    "        try:\n",
    "            publisher=header_info.find(\"publisher\").get_text()\n",
    "        except:\n",
    "            publisher=\"\"\n",
    "\n",
    "        ##date published\n",
    "        try:\n",
    "            date_published=header_info.find(\"date\").get_text()\n",
    "        except:\n",
    "            date_published=\"\"\n",
    "\n",
    "        #authors\n",
    "        try:\n",
    "            forenames=header_info.find_all(\"forename\")\n",
    "            surnames=header_info.find_all(\"surname\")\n",
    "            emails=header_info.find_all(\"email\")\n",
    "            organisations=header_info.find_all(\"orgName\")\n",
    "\n",
    "            persons=[]\n",
    "            for e1,e2,e3,e4 in zip(forenames,surnames,emails,organisations):\n",
    "                persons.append([e1.get_text()+\" \"+e2.get_text(),e3.get_text(),e4.get_text()])\n",
    "        except:\n",
    "            presons=[]\n",
    "            #keywords of the paper\n",
    "        try:\n",
    "            keywords=header_info.keywords\n",
    "            keywords=keywords.find_all(\"term\")\n",
    "            keywords=[element.get_text() for element in keywords]\n",
    "        except:\n",
    "            keywords=[]\n",
    "\n",
    "\n",
    "\n",
    "        #abstract information text\n",
    "        try:\n",
    "            abstract_info=header_info.abstract\n",
    "            abstract=[abstract.get_text() for abstract in abstract_info.find_all(\"s\")]\n",
    "            abstract=\"\".join(abstract)\n",
    "        except:\n",
    "            abstract=\"\"\n",
    "\n",
    "\n",
    "        #resolution and page_count\n",
    "        try:\n",
    "            test=bs_object.facsimile.find_all(\"surface\")\n",
    "            resolution=[[float(element[\"lrx\"]),float(element[\"lry\"])]for element in test]\n",
    "            page_count=len(resolution)\n",
    "        except:\n",
    "            resolution=[]\n",
    "            page_count=[]\n",
    "\n",
    "        res=[title,publisher,date_published,persons,\n",
    "                 keywords,abstract,resolution,page_count]\n",
    "\n",
    "\n",
    "        if(show_results==True):\n",
    "            for element in res:\n",
    "                print(element)\n",
    "\n",
    "        return res\n",
    "            \n",
    "            \n",
    "    def file_mode_execution(self,xml):\n",
    "        \"\"\"returns the header for a particular tei file\"\"\"\n",
    "        try:\n",
    "            bs=self.read_file_xml_to_bs4(xml_file=xml)\n",
    "            header_info=self.extract_header_info(bs)\n",
    "            return [xml,header_info]\n",
    "        except:\n",
    "            pass\n",
    "            return None\n",
    "            \n",
    "    def folder_mode_execution(self):\n",
    "        print(\"running folder mode execution\")\n",
    "        directories_to_consider=[]\n",
    "        for element in os.listdir(directory_path): #gets all the sub directories\n",
    "            xml_state=os.path.exists(directory_path+\"/\"+element+\"/\"+element+\".xml\")\n",
    "            pdf_state=os.path.exists(directory_path+\"/\"+element+\"/\"+element+\".pdf\")\n",
    "            label_state=os.path.exists(directory_path+\"/\"+element+\"/\"+element+\"_annot.xml\")\n",
    "            grobid_state=os.path.exists(directory_path+\"/\"+element+\"/\"+element+\".tei.xml\")\n",
    "            final_state=xml_state and pdf_state and label_state and grobid_state\n",
    "            if(final_state):\n",
    "                directories_to_consider.append(element)\n",
    "                \n",
    "        print(\"The total number of valid pdfs (valid):\" ,len(directories_to_consider))\n",
    "        grobid_xmls=list(map(lambda x: directory_path+\"/\"+x+\"/\"+x+\".tei.xml\" ,directories_to_consider))\n",
    "        print(grobid_xmls[0])\n",
    "        \n",
    "        \n",
    "        #now we need to apply file mode execution to all these files\n",
    "        states=[]\n",
    "        \n",
    "        #the overall task is to read headers from various xml files and hence it can be threaded\n",
    "        res=Parallel(n_jobs=self.n_jobs)(delayed(self.file_mode_execution)(file) for file in tqdm(grobid_xmls))\n",
    "        \n",
    "        \n",
    "    def batch_file_executions(self):\n",
    "        print(self.files_to_run)\n",
    "        res=Parallel(n_jobs=self.n_jobs)(delayed(self.file_mode_execution)(file) for file in tqdm(self.files_to_run))\n",
    "        \n",
    "    def fit(self):\n",
    "        if(self.mode==\"folder\"):\n",
    "            res=self.folder_mode_execution()\n",
    "        elif(self.mode==\"file\"):\n",
    "            res=self.file_mode_execution(self.file_to_run)\n",
    "        else:\n",
    "            res=self.batch_file_executions()\n",
    "        \n",
    "\n",
    "        \n",
    "import time\n",
    "directory_path=\"/Users/mv96/tkb_pdfs\"            \n",
    "res=extract_headers(directory_path,n_jobs=4).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the csv file\n",
    "def clean_label_names(name):\n",
    "    \"\"\"this function assigns class label names into 3 categories\"\"\"\n",
    "    try:\n",
    "        if(str(name)==\"nan\"):\n",
    "            return None\n",
    "        if(isinstance(float(name),float)):\n",
    "            return None\n",
    "    except:\n",
    "        if(name.startswith(\"uri:extthm\")):#it starts with uri\n",
    "            #but it can also be an algorithm or an acknowledgement\n",
    "            if(\"proof\" in name.lower()): #it can be a proof\n",
    "                return \"proof\"\n",
    "            if(\"algorithm\" in name.lower() or \"acknowledgement\" in name.lower()): #it can be a basic block\n",
    "                return \"basic\"\n",
    "            else:\n",
    "                return \"theorem\"\n",
    "        elif(name in [\"basic\",\"overlap\"]):\n",
    "            return name\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "file=successfuls[1]\n",
    "file=\"/Users/mv96/new/dest-pdfs/1812.02037/data.csv\"\n",
    "df=pd.read_csv(file)\n",
    "\n",
    "df[\"label\"]=df[\"label\"].apply(clean_label_names)\n",
    "df[\"label\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "safe=[]\n",
    "unsafe=[]\n",
    "\n",
    "useful_dfs=[]\n",
    "for file in tqdm(successfuls):\n",
    "    \n",
    "    df=pd.read_csv(file)\n",
    "    \n",
    "    df[\"label\"]=df[\"label\"].apply(clean_label_names)\n",
    "    \n",
    "    pdf_labels=df[\"label\"].unique()\n",
    "    \n",
    "    #if the pdf has proof theorem and basic then it's ideal other wise there might be some problems with the \n",
    "    #extraction script\n",
    "    if(\"proof\" in pdf_labels and \n",
    "      \"theorem\" in pdf_labels and\n",
    "      \"basic\" in pdf_labels):\n",
    "        safe.append(file)\n",
    "        selected=df.loc[df['label'].isin([\"proof\",\"basic\",\"theorem\"])] #we dont want overlap and none\n",
    "        useful_dfs.append(selected)\n",
    "    else:\n",
    "        unsafe.append(file)\n",
    "        label=list(df[\"label\"].unique())\n",
    "        if(\"basic\" in label):\n",
    "            label.remove(\"basic\")\n",
    "        if(\"overlap\" in label):\n",
    "            label.remove(\"overlap\")\n",
    "        if(None in label): #generated errors\n",
    "            label.remove(None)\n",
    "            \n",
    "        if(len(label)==1):\n",
    "            filtered=df[df[\"label\"]==label[0]]\n",
    "            useful_dfs.append(filtered)\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "print(len(safe))\n",
    "print(len(unsafe))\n",
    "print(len(useful_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23164b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat(useful_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1db1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/mv96/Desktop/dataset_tkb/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26aa793",
   "metadata": {},
   "source": [
    "# Performing the EDA\n",
    "\n",
    "#### Task done\n",
    "\n",
    "1. Plotting the distribution of font vectors obtaiend over .... pdfs to see if there is an intersting seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_all_data=\"/Users/mv96/Desktop/dataset_tkb/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data frame from this location\n",
    "\n",
    "csvs=pd.read_csv(path_for_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_cols=list(filter(lambda x: \"Unnamed\" not in x, csvs))\n",
    "filtered_useless_cols=csvs[csv_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_useless_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e30fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/9klk1nzn2lz690wvdfb85s_00000gn/T/ipykernel_49752/1984661072.py:2: RuntimeWarning: invalid value encountered in log10\n",
      "  np.log10([1.0,-2.5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0., nan])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log10([1.0,-2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e94742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
