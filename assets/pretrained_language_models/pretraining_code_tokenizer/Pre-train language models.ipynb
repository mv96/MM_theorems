{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28502,
     "status": "ok",
     "timestamp": 1663871531698,
     "user": {
      "displayName": "Shrey Mishra",
      "userId": "13121650155148305561"
     },
     "user_tz": -120
    },
    "id": "x8TQTipgYHBY",
    "outputId": "ba351ef8-7c5c-4795-ceb9-d38c9873c2e5"
   },
   "outputs": [],
   "source": [
    "# Install `transformers` from master\n",
    "#!pip install git+https://github.com/huggingface/transformers\n",
    "#!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1663871532292,
     "user": {
      "displayName": "Shrey Mishra",
      "userId": "13121650155148305561"
     },
     "user_tz": -120
    },
    "id": "hH8m-wabZIJ0",
    "outputId": "b1897d2e-2594-4760-d75f-ea70567dfc5f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff385008fc74c46b33c8b34b020f373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fe89569dcc47978a5f482c1d9f0a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddd666ddd6a43a092679c141fa3e82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f95d82da157496a9f0aa347afa20b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is roberta_default/cc-bert same ?? : False\n",
      "comparing roberta_default vs ccbert the tokenized texts:\n",
      "['Cor', 'oll', 'ary', 'Ġ1', '.', 'ĠLet', 'Ġm', 'Ġ', 'ô', 'ı', '°', '®', 'Ġk', ',', 'Ġand', 'ĠPS', 'Ġbe', 'Ġa', 'Ġdistribution', 'Ġover', 'ĠR', 'm', 'ÃĹ', 'n', 'Ġsatisfying', 'ĠAss', 'umption', 'Ġ1', 'Ġ.', 'ĠThen', ',', 'Ċ', 'E', 'ĠâĪ', '¥', 'âĪ', 'Ĩ', 'T', 'ĠâĪ', '¥', '2', 'Ġ=', 'Ċ', 'Proof', '.', 'ĠThe', 'Ġproof', 'Ġof', 'ĠThe', 'orem', 'Ġ1', 'Ġis', 'Ġdeferred', 'Ġto', 'ĠAppendix', 'ĠA', '.', '1', 'Ġ.', 'Ċ', 'Ġt', '=', '0', 'Ċ', 'for', 'any', 'T', 'ô', 'ı', '°', '®', '0', ',', 'the', 'minimum', 'error', 'E', 'âĪ', '¥', 'âĪ', 'Ĩ', 'T', 'âĪ', '¥', '2', 'is', 'ob', 'tained', 'by', 'cho', 'osing', 'Î¼', 't', '=', 'Î', '¸', '1', 'Ġfor', 'all', '0', 'ô', 'ı', '°', 'Ń', 't', '<', 'T', '.', 'Ċ', 'C', 'onsequ', 'ently', ',']\n",
      "['Corollary', 'Ġ1', '.', 'ĠLet', 'Ġm', 'Ġ', 'ô', 'ı', '°', '®', 'Ġk', ',', 'Ġand', 'ĠPS', 'Ġbe', 'Ġa', 'Ġdistribution', 'Ġover', 'ĠRm', 'ÃĹ', 'n', 'Ġsatisfying', 'ĠAssumption', 'Ġ1', 'Ġ.', 'ĠThen', ',', 'Ċ', 'E', 'ĠâĪ¥âĪĨ', 'T', 'ĠâĪ¥', '2', 'Ġ=', 'Ċ', 'Proof', '.', 'ĠThe', 'Ġproof', 'Ġof', 'ĠTheorem', 'Ġ1', 'Ġis', 'Ġdeferred', 'Ġto', 'ĠAppendix', 'ĠA', '.', '1', 'Ġ.', 'Ċ', 'Ġt', '=', '0', 'Ċ', 'for', 'any', 'T', 'ô', 'ı', '°', '®', '0', ',', 'the', 'minim', 'umer', 'ror', 'E', 'âĪ¥âĪĨ', 'T', 'âĪ¥', '2', 'is', 'obtained', 'by', 'choosing', 'Î¼', 't', '=', 'Î¸', '1', 'Ġforall', '0', 'ô', 'ı', '°', 'Ń', 't', '<', 'T', '.', 'Ċ', 'Consequently', ',']\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer which is fast\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_fn = \"./vocabularies/trained_bpe/tokenizer.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(model_fn)\n",
    "# auto_tokenizer=AutoTokenizer.from_pretrained(model_fn)\n",
    "\n",
    "# tokenize for sample sentence\n",
    "text = \"\"\"Corollary 1. Let m 􏰮 k, and PS be a distribution over Rm×n satisfying Assumption 1 . Then,\n",
    "E ∥∆T ∥2 =\n",
    "Proof. The proof of Theorem 1 is deferred to Appendix A.1 .\n",
    " t=0\n",
    "foranyT􏰮0,theminimumerrorE∥∆T∥2isobtainedbychoosingμt=θ1 forall0􏰭t<T.\n",
    "Consequently,\"\"\"\n",
    "\n",
    "# lets tokenize with our tokenizer\n",
    "cc_tokens = tokenizer.encode(text).tokens  ## cc tokenizer\n",
    "\n",
    "model_dir = \"roberta-base\"\n",
    "# lets look at what the default bert/distibert tokenizer would tokenize\n",
    "# bert_tokenizer = AutoTokenizer.from_pretrained(model_fn)\n",
    "# bert_tokens=bert_tokenizer.tokenize(text)\n",
    "\n",
    "# lets look at distilbert\n",
    "roberta_default_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "roberta_default_tokens = roberta_default_tokenizer.tokenize(text)\n",
    "\n",
    "# is distilbert equal to bert for this example\n",
    "# print(\"is bert/distilbert same ?? :\",distilbert_tokens==bert_tokens)\n",
    "\n",
    "# is distilbert equal to our ccbert for this example\n",
    "print(\"is roberta_default/cc-bert same ?? :\", cc_tokens == roberta_default_tokens)\n",
    "\n",
    "# which tokens are same and which are different?\n",
    "print(\"comparing roberta_default vs ccbert the tokenized texts:\")\n",
    "print(roberta_default_tokens)\n",
    "print(cc_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# vocab.json to vocab.txt\n",
    "import json\n",
    "with open(model_fn) as f:\n",
    "    d = json.load(f)\n",
    "    \n",
    "vocab = d['model']['vocab']\n",
    "vocab_txt = ''\n",
    "for k, v in vocab.items():\n",
    "    vocab_txt += k\n",
    "    vocab_txt += '\\n'\n",
    "    \n",
    "vocab_txt = vocab_txt[:-1]\n",
    "    \n",
    "with open('./vocab.txt', 'wt') as f:\n",
    "    f.write(vocab_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1663871532818,
     "user": {
      "displayName": "Shrey Mishra",
      "userId": "13121650155148305561"
     },
     "user_tz": -120
    },
    "id": "T44U8F7PFq3n",
    "outputId": "687dd189-8df5-441d-fa52-9c2de951d651"
   },
   "outputs": [],
   "source": [
    "# we cant see the tokenizer cls embedding to see that lets load again\n",
    "\n",
    "from tokenizers.implementations import BertWordPieceTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/vocabularies/trained_tokenizer/vocab.txt\"\n",
    ")\n",
    "\n",
    "tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Corollary',\n",
       " 'Ġ1',\n",
       " '.',\n",
       " 'ĠLet',\n",
       " 'Ġm',\n",
       " 'Ġ',\n",
       " 'ô',\n",
       " 'ı',\n",
       " '°',\n",
       " '®',\n",
       " 'Ġk',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'ĠPS',\n",
       " 'Ġbe',\n",
       " 'Ġa',\n",
       " 'Ġdistribution',\n",
       " 'Ġover',\n",
       " 'ĠRm',\n",
       " 'ÃĹ',\n",
       " 'n',\n",
       " 'Ġsatisfying',\n",
       " 'ĠAssumption',\n",
       " 'Ġ1',\n",
       " 'Ġ.',\n",
       " 'ĠThen',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'E',\n",
       " 'ĠâĪ¥âĪĨ',\n",
       " 'T',\n",
       " 'ĠâĪ¥',\n",
       " '2',\n",
       " 'Ġ=',\n",
       " 'Ċ',\n",
       " 'Proof',\n",
       " '.',\n",
       " 'ĠThe',\n",
       " 'Ġproof',\n",
       " 'Ġof',\n",
       " 'ĠTheorem',\n",
       " 'Ġ1',\n",
       " 'Ġis',\n",
       " 'Ġdeferred',\n",
       " 'Ġto',\n",
       " 'ĠAppendix',\n",
       " 'ĠA',\n",
       " '.',\n",
       " '1',\n",
       " 'Ġ.',\n",
       " 'Ċ',\n",
       " 'Ġt',\n",
       " '=',\n",
       " '0',\n",
       " 'Ċ',\n",
       " 'for',\n",
       " 'any',\n",
       " 'T',\n",
       " 'ô',\n",
       " 'ı',\n",
       " '°',\n",
       " '®',\n",
       " '0',\n",
       " ',',\n",
       " 'the',\n",
       " 'minim',\n",
       " 'umer',\n",
       " 'ror',\n",
       " 'E',\n",
       " 'âĪ¥âĪĨ',\n",
       " 'T',\n",
       " 'âĪ¥',\n",
       " '2',\n",
       " 'is',\n",
       " 'obtained',\n",
       " 'by',\n",
       " 'choosing',\n",
       " 'Î¼',\n",
       " 't',\n",
       " '=',\n",
       " 'Î¸',\n",
       " '1',\n",
       " 'Ġforall',\n",
       " '0',\n",
       " 'ô',\n",
       " 'ı',\n",
       " '°',\n",
       " 'Ń',\n",
       " 't',\n",
       " '<',\n",
       " 'T',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Consequently',\n",
       " ',',\n",
       " '</s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we cant see the tokenizer cls embedding to see that lets load again\n",
    "\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./vocabularies/roberta_pre_trained_tokenizer/vocab.json\",\n",
    "    \"./vocabularies/roberta_pre_trained_tokenizer/merges.txt\",\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1663871535058,
     "user": {
      "displayName": "Shrey Mishra",
      "userId": "13121650155148305561"
     },
     "user_tz": -120
    },
    "id": "8sOsyEGuDPc9",
    "outputId": "a6999321-b005-4d62-dad7-05b7f7cc71ee"
   },
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "model_name=\"/linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_distilbert-base-uncased\"\n",
    "config=AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model=AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "no of devices: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "devices = strategy.num_replicas_in_sync\n",
    "print(\"no of devices: {}\".format(devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "\n",
    "model_name = \"/linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_bert-base-uncased\"\n",
    "with strategy.scope():\n",
    "    model = TFBertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_roberta-base/ were not used when initializing TFRobertaForMaskedLM: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForMaskedLM were not initialized from the model checkpoint at /linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_roberta-base/ and are newly initialized: ['lm_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFRobertaForMaskedLM\n",
    "\n",
    "model_name = \"/linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_roberta-base/\"\n",
    "with strategy.scope():\n",
    "    model = TFRobertaForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1663871535059,
     "user": {
      "displayName": "Shrey Mishra",
      "userId": "13121650155148305561"
     },
     "user_tz": -120
    },
    "id": "NKIA36ZwvghB",
    "outputId": "ffa11f04-1b34-4c98-a73e-7f21601889c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124697433\n"
     ]
    }
   ],
   "source": [
    "# we can see the full mlm model for bert here\n",
    "print(model.num_parameters())\n",
    "# lp=list(model.parameters())\n",
    "# print(len(lp)) #number of matrices and vectors involved in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM at 0x148bd5e362f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfuCCO8hedk1"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/vocabularies/trained_tokenizer\",\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"./vocabularies/roberta_pre_trained_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-77d85291877c4913\n",
      "Reusing dataset text (/linkhome/rech/gennsp01/uyf36me/.cache/huggingface/datasets/text/default-77d85291877c4913/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3671213207d54c2383a0351e57806cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset=load_dataset(\"/Users/mv96/Downloads/full_text_data.txt\")\n",
    "#provide the full\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": [\n",
    "            \"/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/full_text_data.txt\"\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x153cad163f40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a168d46c50e046cab85d79c6d3f7e837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3bc5b926d464fba0cb14e0081e4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3efcab51814afaa840b30235952296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f211f11afd42439a68566aa1d33c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6d135169e948ca97bea469b8910f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdedeb73c6b445eaf66c73d76f47f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6039ab5edfb74795ae8ea09cc499ba0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, num_proc=7, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  1668,\n",
       "  4533,\n",
       "  2376,\n",
       "  4432,\n",
       "  2361,\n",
       "  967,\n",
       "  1576,\n",
       "  367,\n",
       "  40369,\n",
       "  591,\n",
       "  2468,\n",
       "  22530,\n",
       "  823,\n",
       "  294,\n",
       "  267,\n",
       "  3936,\n",
       "  2361,\n",
       "  6889,\n",
       "  294,\n",
       "  2358,\n",
       "  299,\n",
       "  7289,\n",
       "  4432,\n",
       "  2361,\n",
       "  1756,\n",
       "  299,\n",
       "  935,\n",
       "  4201,\n",
       "  294,\n",
       "  5425,\n",
       "  2361,\n",
       "  1720,\n",
       "  18,\n",
       "  1668,\n",
       "  967,\n",
       "  921,\n",
       "  2175,\n",
       "  17,\n",
       "  5109,\n",
       "  45551,\n",
       "  9456,\n",
       "  2438,\n",
       "  2739,\n",
       "  3386,\n",
       "  3777,\n",
       "  299,\n",
       "  6810,\n",
       "  2739,\n",
       "  3562,\n",
       "  1944,\n",
       "  925,\n",
       "  803,\n",
       "  2450,\n",
       "  1963,\n",
       "  31,\n",
       "  2545,\n",
       "  16,\n",
       "  431,\n",
       "  1963,\n",
       "  301,\n",
       "  6249,\n",
       "  21123,\n",
       "  299,\n",
       "  740,\n",
       "  267,\n",
       "  5130,\n",
       "  2450,\n",
       "  1963,\n",
       "  2960,\n",
       "  301,\n",
       "  1033,\n",
       "  294,\n",
       "  3844,\n",
       "  18,\n",
       "  2034,\n",
       "  2062,\n",
       "  262,\n",
       "  1397,\n",
       "  967,\n",
       "  16,\n",
       "  4720,\n",
       "  42541,\n",
       "  17025,\n",
       "  16,\n",
       "  319,\n",
       "  14554,\n",
       "  684,\n",
       "  267,\n",
       "  802,\n",
       "  899,\n",
       "  853,\n",
       "  610,\n",
       "  3235,\n",
       "  299,\n",
       "  599,\n",
       "  319,\n",
       "  431,\n",
       "  967,\n",
       "  435,\n",
       "  3359,\n",
       "  929,\n",
       "  2438,\n",
       "  2739,\n",
       "  925,\n",
       "  2450,\n",
       "  40369,\n",
       "  1491,\n",
       "  10258,\n",
       "  267,\n",
       "  271,\n",
       "  45544,\n",
       "  11442,\n",
       "  13,\n",
       "  476,\n",
       "  12,\n",
       "  82,\n",
       "  378,\n",
       "  307,\n",
       "  3040,\n",
       "  2055,\n",
       "  282,\n",
       "  2175,\n",
       "  17,\n",
       "  5109,\n",
       "  40369,\n",
       "  321,\n",
       "  1172,\n",
       "  293,\n",
       "  18,\n",
       "  14553,\n",
       "  42541,\n",
       "  17025,\n",
       "  3531,\n",
       "  5580,\n",
       "  2175,\n",
       "  17,\n",
       "  5109,\n",
       "  4733,\n",
       "  294,\n",
       "  262,\n",
       "  2295,\n",
       "  17,\n",
       "  1484,\n",
       "  2667,\n",
       "  1491,\n",
       "  5823,\n",
       "  2450,\n",
       "  4733,\n",
       "  294,\n",
       "  267,\n",
       "  6736,\n",
       "  2667,\n",
       "  18,\n",
       "  2034,\n",
       "  2743,\n",
       "  4720,\n",
       "  42541,\n",
       "  17025,\n",
       "  921,\n",
       "  5571,\n",
       "  321,\n",
       "  6039,\n",
       "  7346,\n",
       "  4755,\n",
       "  294,\n",
       "  633,\n",
       "  282,\n",
       "  267,\n",
       "  6258,\n",
       "  632,\n",
       "  22,\n",
       "  1056,\n",
       "  299,\n",
       "  267,\n",
       "  632,\n",
       "  21,\n",
       "  1056,\n",
       "  16,\n",
       "  4402,\n",
       "  319,\n",
       "  803,\n",
       "  2055,\n",
       "  785,\n",
       "  2741,\n",
       "  301,\n",
       "  996,\n",
       "  294,\n",
       "  267,\n",
       "  1172,\n",
       "  16,\n",
       "  299,\n",
       "  2881,\n",
       "  3777,\n",
       "  321,\n",
       "  267,\n",
       "  967,\n",
       "  1303,\n",
       "  282,\n",
       "  267,\n",
       "  1773,\n",
       "  282,\n",
       "  6039,\n",
       "  13412,\n",
       "  18,\n",
       "  10004,\n",
       "  16,\n",
       "  335,\n",
       "  599,\n",
       "  7158,\n",
       "  929,\n",
       "  350,\n",
       "  2264,\n",
       "  5003,\n",
       "  18,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1668,\n",
       " 4533,\n",
       " 2376,\n",
       " 4432,\n",
       " 2361,\n",
       " 967,\n",
       " 1576,\n",
       " 367,\n",
       " 40369,\n",
       " 591,\n",
       " 2468,\n",
       " 22530,\n",
       " 823,\n",
       " 294,\n",
       " 267,\n",
       " 3936,\n",
       " 2361,\n",
       " 6889,\n",
       " 294,\n",
       " 2358,\n",
       " 299,\n",
       " 7289,\n",
       " 4432,\n",
       " 2361,\n",
       " 1756,\n",
       " 299,\n",
       " 935,\n",
       " 4201,\n",
       " 294,\n",
       " 5425,\n",
       " 2361,\n",
       " 1720,\n",
       " 18,\n",
       " 1668,\n",
       " 967,\n",
       " 921,\n",
       " 2175,\n",
       " 17,\n",
       " 5109,\n",
       " 45551,\n",
       " 9456,\n",
       " 2438,\n",
       " 2739,\n",
       " 3386,\n",
       " 3777,\n",
       " 299,\n",
       " 6810,\n",
       " 2739,\n",
       " 3562,\n",
       " 1944,\n",
       " 925,\n",
       " 803,\n",
       " 2450,\n",
       " 1963,\n",
       " 31,\n",
       " 2545,\n",
       " 16,\n",
       " 431,\n",
       " 1963,\n",
       " 301,\n",
       " 6249,\n",
       " 21123,\n",
       " 299,\n",
       " 740,\n",
       " 267,\n",
       " 5130,\n",
       " 2450,\n",
       " 1963,\n",
       " 2960,\n",
       " 301,\n",
       " 1033,\n",
       " 294,\n",
       " 3844,\n",
       " 18,\n",
       " 2034,\n",
       " 2062,\n",
       " 262,\n",
       " 1397,\n",
       " 967,\n",
       " 16,\n",
       " 4720,\n",
       " 42541,\n",
       " 17025,\n",
       " 16,\n",
       " 319,\n",
       " 14554,\n",
       " 684,\n",
       " 267,\n",
       " 802,\n",
       " 899,\n",
       " 853,\n",
       " 610,\n",
       " 3235,\n",
       " 299,\n",
       " 599,\n",
       " 319,\n",
       " 431,\n",
       " 967,\n",
       " 435,\n",
       " 3359,\n",
       " 929,\n",
       " 2438,\n",
       " 2739,\n",
       " 925,\n",
       " 2450,\n",
       " 40369,\n",
       " 1491,\n",
       " 10258,\n",
       " 267,\n",
       " 271,\n",
       " 45544,\n",
       " 11442,\n",
       " 13,\n",
       " 476,\n",
       " 12,\n",
       " 82,\n",
       " 378,\n",
       " 307,\n",
       " 3040,\n",
       " 2055,\n",
       " 282,\n",
       " 2175,\n",
       " 17,\n",
       " 5109,\n",
       " 40369,\n",
       " 321,\n",
       " 1172,\n",
       " 293,\n",
       " 18,\n",
       " 14553,\n",
       " 42541,\n",
       " 17025,\n",
       " 3531,\n",
       " 5580,\n",
       " 2175,\n",
       " 17,\n",
       " 5109,\n",
       " 4733,\n",
       " 294,\n",
       " 262,\n",
       " 2295,\n",
       " 17,\n",
       " 1484,\n",
       " 2667,\n",
       " 1491,\n",
       " 5823,\n",
       " 2450,\n",
       " 4733,\n",
       " 294,\n",
       " 267,\n",
       " 6736,\n",
       " 2667,\n",
       " 18,\n",
       " 2034,\n",
       " 2743,\n",
       " 4720,\n",
       " 42541,\n",
       " 17025,\n",
       " 921,\n",
       " 5571,\n",
       " 321,\n",
       " 6039,\n",
       " 7346,\n",
       " 4755,\n",
       " 294,\n",
       " 633,\n",
       " 282,\n",
       " 267,\n",
       " 6258,\n",
       " 632,\n",
       " 22,\n",
       " 1056,\n",
       " 299,\n",
       " 267,\n",
       " 632,\n",
       " 21,\n",
       " 1056,\n",
       " 16,\n",
       " 4402,\n",
       " 319,\n",
       " 803,\n",
       " 2055,\n",
       " 785,\n",
       " 2741,\n",
       " 301,\n",
       " 996,\n",
       " 294,\n",
       " 267,\n",
       " 1172,\n",
       " 16,\n",
       " 299,\n",
       " 2881,\n",
       " 3777,\n",
       " 321,\n",
       " 267,\n",
       " 967,\n",
       " 1303,\n",
       " 282,\n",
       " 267,\n",
       " 1773,\n",
       " 282,\n",
       " 6039,\n",
       " 13412,\n",
       " 18,\n",
       " 10004,\n",
       " 16,\n",
       " 335,\n",
       " 599,\n",
       " 7158,\n",
       " 929,\n",
       " 350,\n",
       " 2264,\n",
       " 5003,\n",
       " 18,\n",
       " 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the sentence back from the tokens\n",
    "tokens = tokenized_datasets[\"train\"][1][\"input_ids\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>The adaptive gradient online learning method known as AdaGrad has seen widespread use in the machine learning community in stochastic and adversarial online learning problems and more recently in deep learning methods.The method's full-matrix incarnation offers much better theoretical guarantees and potentially better empirical performance than its diagonal version; however, this version is computationally prohibitive and so the simpler diagonal version often is used in practice.We introduce a new method, CompAdaGrad, that navigates the space between these two schemes and show that this method can yield results much better than diagonal AdaGrad while avoiding the (effectively intractable) O(n 3 ) computational complexity of full-matrix AdaGrad for dimension n.CompAdaGrad essentially performs full-matrix regularization in a low-dimensional subspace while performing diagonal regularization in the complementary subspace.We derive CompAdaGrad's updates for composite mirror descent in case of the squared ℓ2 norm and the ℓ1 norm, demonstrate that its complexity per iteration is linear in the dimension, and establish guarantees for the method independent of the choice of composite regularizer.Finally, we show preliminary results on several datasets.</s>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 3\n"
     ]
    }
   ],
   "source": [
    "t1 = tokenized_datasets[\"train\"][1][\"input_ids\"]\n",
    "t2 = tokenized_datasets[\"train\"][2][\"input_ids\"]\n",
    "print(len(t1), len(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  6 17:08:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:39:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 250W |  39085MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 250W |  38819MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 250W |  38819MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 250W |  38819MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    241773      C   ...w-2.9.1+py3.10/bin/python    39083MiB |\n",
      "|    1   N/A  N/A    241773      C   ...w-2.9.1+py3.10/bin/python    38817MiB |\n",
      "|    2   N/A  N/A    241773      C   ...w-2.9.1+py3.10/bin/python    38817MiB |\n",
      "|    3   N/A  N/A    241773      C   ...w-2.9.1+py3.10/bin/python    38817MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9f339e967540ce928a6923d7bdbd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdaf07edfb54d59b7a30c8a71db10e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ee559e24a4c5486878b2db89fc2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b71b82c218425692be436239c9d447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0ceb1865e845da8b5e90a5a4f10b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce434dd1f9c49a49a0337c0b0b6f632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc672202d45b4033876510a9ba8bd465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/7802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "# acts as max length trimming\n",
    "def group_texts(examples):\n",
    "    \"\"\"source -\n",
    "    https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb#scrollTo=OytOGDhz4OXz\n",
    "    \"\"\"\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12229818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets.save_to_disk(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 12229818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "lm_datasets = load_from_disk(\"test\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, TFAutoModelForMaskedLM\n",
    "\n",
    "# model_checkpoint = \"distilbert-base-cased\"\n",
    "# config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = TFAutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "\n",
    "# LAMB optimizer\n",
    "from tensorflow_addons.optimizers import LAMB\n",
    "\n",
    "optimizer_2 = LAMB(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with strategy.scope():\n",
    "    model.compile(optimizer=optimizer_2)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to track\n",
    "batch_size = 64 * devices  # 64 for a100\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "# model_name=\"/linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = (\n",
    "    lm_datasets[\"train\"]\n",
    "    .to_tf_dataset(\n",
    "        columns=[\"attention_mask\", \"input_ids\", \"labels\"],  # token_type_ids for bert\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"Enter your api key here\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "# set up wandb\n",
    "run = wandb.init(\n",
    "    mode=\"offline\",\n",
    "    project=\"Transformers_pretrain\",\n",
    "    config={  ## hyper params and meta data\n",
    "        \"model_name\": model_name,  # we can train the tokenizer of cased,\n",
    "        \"num_epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"start_point\": \"bert-uncased-mlm_ep00\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/roberta-mlm_ep_9'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_right_path(\n",
    "    mname=None, default_model_name=\"roberta-mlm_ep_\"\n",
    "):  # change name for bert\n",
    "    if (mname) == None:\n",
    "        pwd = os.getcwd()\n",
    "    mname = os.path.join(pwd, default_model_name)\n",
    "    # print(default_model_name)\n",
    "    sub_selected = list(\n",
    "        filter(lambda x: x.startswith(default_model_name), list(os.listdir(pwd)))\n",
    "    )\n",
    "\n",
    "    def last_2chars(x):\n",
    "        return x[-2:]\n",
    "\n",
    "    sub_selected = sorted(sub_selected, key=last_2chars)\n",
    "    if len(sub_selected) == 0:\n",
    "        return\n",
    "\n",
    "    return os.path.join(pwd, sub_selected[-1])\n",
    "\n",
    "\n",
    "get_right_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/roberta-mlm_ep_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at /gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/roberta-mlm_ep_9.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 200 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 200 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "47773/47773 [==============================] - 57921s 1s/step - loss: 1.0571\n",
      "/gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/roberta-mlm_ep_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at /gpfsdswork/projects/rech/zpf/uyf36me/pretraining_language_model/roberta-mlm_ep_9.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 200 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      " 9469/47773 [====>.........................] - ETA: 12:51:55 - loss: 1.0671"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./mlm_from_scratch_model_save/logs\")\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "\n",
    "iterations = 3\n",
    "\n",
    "\n",
    "for _ in range(iterations):\n",
    "    try:\n",
    "        model_path = get_right_path()\n",
    "        iter_no_save = int(model_path.rsplit(\"/\", 1)[-1].rsplit(\"_\")[-1]) + 1\n",
    "    except:  # for offline run\n",
    "        model_path = \"/linkhome/rech/gennsp01/uyf36me/work/NLP_Classification_of_proofs/transformers_offline/transformer_roberta-base/\"\n",
    "        iter_no_save = _\n",
    "\n",
    "    print(model_path)\n",
    "    with strategy.scope():\n",
    "        model = TFRobertaForMaskedLM.from_pretrained(model_path)\n",
    "        model.compile(optimizer=optimizer_2)  # optimizer 2 is lamb\n",
    "        model.fit(tf_train_dataset, verbose=1, epochs=1, callbacks=callbacks)\n",
    "        model.save_pretrained(\"./\" + \"roberta-mlm_ep_{}\".format(iter_no_save))\n",
    "    # distil #2.13 #1.46 #1.3787\n",
    "    # bert #2.84   #1.7012 #1.49  #1.3069 #1.2554 #1.2156\n",
    "    # 1.1841 #1.1582 #1.138 #1.1221 #1.11(final)\n",
    "    # roberta #2.4246 #1.61   #1.4191  #1.3134  #1.2436 #1.19\n",
    "    # 1.1545 #1.1232 #1.0975  #1.06    #1.05\n",
    "\n",
    "# distilbert\n",
    "# 23 hours on 1 a100\n",
    "# 5 hours on 4 a100\n",
    "\n",
    "# bert training\n",
    "# 4x A100 batch 64\n",
    "# 11 hours on optimizer adamw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size -16\n",
    "# batch size 16 765868 15 hours on one gpu\n",
    "# batch size 16*4 on 191467 12 hours 4 gpus\n",
    "\n",
    "# batch size -32\n",
    "# batch size 32 765868 15 hours on one gpu\n",
    "# batch size 32*4 on 191467 9 hours 4 gpus\n",
    "\n",
    "# batch size 64\n",
    "# batch size 32 765868 15 hours on one gpu\n",
    "# batch size 32*4 on 191467 7 hours 4 gpus\n",
    "\n",
    "# batch size 128\n",
    "# batch size 32 765868 15 hours on one gpu\n",
    "# batch size 32*4 on 23934 6:20m hours 4 gpus\n",
    "\n",
    "# batch size 128 with prefetch\n",
    "# batch size 32 765868 15 hours on one gpu\n",
    "# batch size 32*4 on 23934 5.20m hours 4 gpus\n",
    "\n",
    "# batch size 256 with prefetch\n",
    "# batch size 32 765868 15 hours on one gpu\n",
    "# batch size 32*4 on 23934 5.20m hours 4 gpus"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
