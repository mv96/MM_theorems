{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efd8e2-db0a-4e33-a836-0557925785dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 14:48:47.738216: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a91be-e37a-40ff-a02b-6c0decddefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"/gpfsscratch/rech/zpf/uyf36me/validation_patches/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aca2d14-7c30-4a81-987c-b12dba5884c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314504, 125526, 85803, 3470]\n",
      "ratios in the data --\n",
      "0.5941851831559617\n",
      "0.2371533885128178\n",
      "0.1621056370358755\n",
      "0.006555791295345011\n"
     ]
    }
   ],
   "source": [
    "main_path=\"/gpfsscratch/rech/zpf/uyf36me/validation_patches/\"\n",
    "\n",
    "label_0=os.path.join(main_path,\"label_0/**.png\") #basic\n",
    "label_1=os.path.join(main_path,\"label_1/**.png\") #proof\n",
    "label_2=os.path.join(main_path,\"label_2/**.png\") #theorem\n",
    "label_3=os.path.join(main_path,\"label_3/**.png\") #overlap\n",
    "\n",
    "vals=[len(glob.glob(label_0)),len(glob.glob(label_1)),len(glob.glob(label_2)),len(glob.glob(label_3))]\n",
    "print(vals)\n",
    "print(\"ratios in the data --\")\n",
    "\n",
    "for val in vals:\n",
    "    print(val/sum(vals))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25e1ff1-3559-41ab-9b6a-0b86d2b621fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def white_padding_and_scaling(default_shape,file_loc,overwrite=False):\n",
    "    \"\"\"\n",
    "    2- adds white padding wherever necessary\n",
    "    3- takes bitwise NOT transformation this esentially inverts the image sets black -0 as background while \n",
    "    255 is set as foreground\n",
    "    4- if overwrite true then makes a new file with '_t' suffix \n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_array=cv2.imread(file_loc)\n",
    "        shape=img_array.shape\n",
    "    except:\n",
    "        print(\"error in white padding--\",file_loc)\n",
    "        return\n",
    "\n",
    "    padding_height=0\n",
    "    padding_width=0\n",
    "    crop_width=False\n",
    "    crop_height=False\n",
    "\n",
    "    if(shape[0]<=default_shape[0]): #if img is small in width then we need padding then \n",
    "        padding_height=default_shape[0]-shape[0]\n",
    "    else:\n",
    "        crop_height=True\n",
    "        padding_height=0\n",
    "    if(shape[1]<=default_shape[1]):\n",
    "        padding_width=default_shape[1]-shape[1]\n",
    "    else:\n",
    "        crop_width=True\n",
    "        padding_width=0\n",
    "    if(padding_width>0 or padding_height>0):\n",
    "        colour_fill=(255,255,255) #colour to pad this is white\n",
    "        new_array=cv2.copyMakeBorder(img_array, 0,padding_height , 0, padding_width, cv2.BORDER_CONSTANT,value=colour_fill)\n",
    "    else:\n",
    "        new_array=img_array[0:default_shape[0], 0:default_shape[1]]\n",
    "\n",
    "    if(crop_width==True):\n",
    "        new_array=new_array[0:default_shape[0], 0:default_shape[1]]\n",
    "    if(crop_height==True):\n",
    "        new_array=new_array[0:default_shape[0], 0:default_shape[1]]\n",
    "\n",
    "\n",
    "    new_array=cv2.bitwise_not(new_array)\n",
    "    if(overwrite==True):\n",
    "        new_name=file_loc.replace(\".png\",\"_t.png\")\n",
    "        #print(new_name)\n",
    "        cv2.imwrite(new_name,new_array)\n",
    "        os.remove(file_loc)\n",
    "        return\n",
    "\n",
    "    return new_array\n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ce8c21-5d32-4478-9673-118251884385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "--running transformations\n"
     ]
    }
   ],
   "source": [
    "#generate dataset\n",
    "\n",
    "\n",
    "path=\"/gpfsscratch/rech/zpf/uyf36me/validation_patches/**/**.png\"\n",
    "\n",
    "png_files=glob.glob(path)\n",
    "\n",
    "            \n",
    "filtered_files=list(filter(lambda x: not x.endswith(\"_t.png\"),png_files))\n",
    "print(len(filtered_files))\n",
    "\n",
    "bad_files=list(filter(lambda x:  x.endswith(\"_t_t.png\"),png_files))\n",
    "print(len(bad_files))\n",
    "\n",
    "\n",
    "print(\"--running transformations\")\n",
    "image_shapes=(400,1400)\n",
    "n_jobs=-2\n",
    "#res=Parallel(n_jobs=n_jobs,backend=\"threading\",verbose=2)(delayed(white_padding_and_scaling)\n",
    "                                           #(default_shape=image_shapes,file_loc=fname,overwrite=True) for fname in tqdm(filtered_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "467b1b56-76ac-4589-b0a2-55de74eb568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "no of devices: 4\n"
     ]
    }
   ],
   "source": [
    "strategy=tf.distribute.MirroredStrategy()\n",
    "devices=strategy.num_replicas_in_sync\n",
    "\n",
    "print(\"no of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf708720-9485-45a2-922b-b04ee8605c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 529303 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "image_shapes=(400,1400)\n",
    "batch_per_gpu=4\n",
    "batch_size=batch_per_gpu*devices\n",
    "\n",
    "sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"/gpfsscratch/rech/zpf/uyf36me/validation_patches\",\n",
    "    image_size=image_shapes,\n",
    "    batch_size=batch_size,\n",
    "    seed=2,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    shuffle=False #<<<<<<<<change this when training\n",
    "    )\n",
    "\n",
    "sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f217f641-e37f-4561-87d1-e4a8cf57707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33082/33082 [16:57<00:00, 32.51it/s]\n"
     ]
    }
   ],
   "source": [
    "labels=None\n",
    "for x, y in tqdm(sub_sample_validation_dataset):\n",
    "    if(labels is None):\n",
    "        labels=y\n",
    "    else:\n",
    "        labels=np.concatenate([labels,y])\n",
    "        \n",
    "#ground truth        \n",
    "y_true=np.argmax(labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "055441bd-cf6d-4481-af13-901b155d6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 400, 1400, 3)      0         \n",
      "                                                                 \n",
      " efficientnetv2-m (Functiona  (None, 1280)             53150388  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 5124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,155,512\n",
      "Trainable params: 52,863,480\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:09:17.055429: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 529303\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33082/33082 [==============================] - 3067s 92ms/step\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfslocalsup/pub/anaconda-py3/2022.05/envs/tensorflow-gpu-2.11.0+py3.10.8/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfslocalsup/pub/anaconda-py3/2022.05/envs/tensorflow-gpu-2.11.0+py3.10.8/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Basic       0.69      0.91      0.79    314504\n",
      "       Proof       0.61      0.22      0.33    125526\n",
      "     Theorem       0.78      0.62      0.69     85803\n",
      "     Overlap       0.00      0.00      0.00      3470\n",
      "\n",
      "    accuracy                           0.69    529303\n",
      "   macro avg       0.52      0.44      0.45    529303\n",
      "weighted avg       0.68      0.69      0.66    529303\n",
      "\n",
      "f1 score of the new_models/r_efficientnetv2m_avg9.h5 is 0.44971657487054234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfslocalsup/pub/anaconda-py3/2022.05/envs/tensorflow-gpu-2.11.0+py3.10.8/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tensorflow_addons.optimizers import AdamW ,LAMB\n",
    "\n",
    "def evaluate_f1_for_tf_model(model_path,validation_dataset,y_true,show_confusion_report=True):\n",
    "    \n",
    "    #460h cpu for 28K images\n",
    "    # 4 A100 can do the job in\n",
    "\n",
    "    class_names=[\"Basic\",\"Proof\",\"Theorem\",\"Overlap\"]\n",
    "\n",
    "    # Wrap the loaded model inside the strategy scope to distribute it across the GPUs\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    #show model arch\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    #generating predictions\n",
    "    predictions=model.predict(validation_dataset)\n",
    "    \n",
    "    #generating predictions\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    if(show_confusion_report is True):\n",
    "        print('Confusion Matrix')\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "        \n",
    "    return f1_score(y_true,y_pred,average=\"macro\")\n",
    "    \n",
    "#\"EfficientNetB0.h5\",\"EfficientNetB0_max.h5\",\"EfficientNetB0_avg.h5\",\n",
    "        #\"EfficientNetB4_avg.h5\",\"efficientnetv2s_avg.h5\",\n",
    "models=[\"new_models/r_efficientnetv2m_avg9.h5\"]\n",
    "\n",
    "for model in models:\n",
    "    _f1_score=evaluate_f1_for_tf_model(model_path=model,validation_dataset=sub_sample_validation_dataset,y_true=y_true)\n",
    "    print(f\"f1 score of the {model} is {_f1_score}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e208781-67f6-4f2b-ac45-a9b15da25c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import AdamW ,LAMB\n",
    "#460h cpu for 28K images\n",
    "# 4 A100 can do the job in\n",
    "\n",
    "class_names=[\"Basic\",\"Proof\",\"Theorem\",\"Overlap\"]\n",
    "model_path=\"efficientnetv2s_avg.h5\"\n",
    "\n",
    "# Wrap the loaded model inside the strategy scope to distribute it across the GPUs\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8ea4d-e0af-4660-9d96-3af2212676cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(sub_sample_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489f52c-3c12-46c9-bfd1-906dd8ddf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41568c5-3154-4823-b306-b3a5a65d4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7eee77-9739-44ee-8f10-c2d1e3c9e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.argmax(labels,axis=1)\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050b3a95-bff7-41a1-bc86-810b8e0162dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[123291   1248  34516     16]\n",
      " [ 47168    987  16129      0]\n",
      " [  1812     35   5401     18]\n",
      " [   955      6    222      1]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Basic       0.71      0.78      0.74    159071\n",
      "      Proofs       0.43      0.02      0.03     64284\n",
      "    Theorems       0.10      0.74      0.17      7266\n",
      "    Overlaps       0.03      0.00      0.00      1184\n",
      "\n",
      "    accuracy                           0.56    231805\n",
      "   macro avg       0.32      0.38      0.24    231805\n",
      "weighted avg       0.61      0.56      0.52    231805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = ['Basic', 'Proofs', 'Theorems','Overlaps']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d7a0b-5f56-4ac4-a338-fcb8b4110de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model on larger dataset to see performance difference\n",
    "#use F1 score to measure the impact\n",
    "#decide the pooling part\n",
    "#do big arch lead to bad generalization\n",
    "#flops vs accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu-2.11.0_py3.10.8",
   "language": "python",
   "name": "module-conda-env-tensorflow-gpu-2.11.0_py3.10.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
