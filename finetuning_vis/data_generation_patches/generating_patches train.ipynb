{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fbb18-45a4-4ce4-b865-a651eec1c344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list the batch folder where all pdfs are \n",
    "#the folder\n",
    "\n",
    "train_batches=\"/gpfsscratch/rech/zpf/uyf36me/training_batches\"\n",
    "\n",
    "import os\n",
    "\n",
    "order=['batch_28',\n",
    " 'batch_18',\n",
    " 'batch_7',\n",
    " 'batch_2',\n",
    " 'batch_0',\n",
    " 'batch_108',\n",
    " 'batch_106',\n",
    " 'batch_85',\n",
    " 'batch_123',\n",
    " 'batch_48',\n",
    " 'batch_114',\n",
    " 'batch_125',\n",
    " 'batch_67',\n",
    " 'batch_4',\n",
    " 'batch_93',\n",
    " 'batch_3',\n",
    " 'batch_86',\n",
    " 'batch_36',\n",
    " 'batch_41',\n",
    " 'batch_100']\n",
    "\n",
    "batches=[os.path.join(train_batches,batch) for batch in order]\n",
    "\n",
    "\n",
    "#reordering file name accroding to some predefined run (which is here nlp)\n",
    "train_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/**.csv\"\n",
    "import glob\n",
    "files=glob.glob(train_path)\n",
    "\n",
    "file_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/roberta_from_scratch_ft.txt\"\n",
    "\n",
    "import os\n",
    "base=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/\"\n",
    "with open(file_path,\"r\") as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "start=[]\n",
    "for line in lines:\n",
    "    val=line.split(\",\")[0]\n",
    "    start.append(val)\n",
    "\n",
    "start=files\n",
    "start\n",
    "# print(start)\n",
    "        \n",
    "batches,csv_files=batches,files\n",
    "\n",
    "#align csvs\n",
    "csv_order=[]\n",
    "for batch in batches:\n",
    "    \n",
    "    batch_name=batch.rsplit(\"/\")[-1]\n",
    "\n",
    "    \n",
    "    for element in csv_files:\n",
    "        if(batch_name+\".csv\" in element):\n",
    "            \n",
    "            csv_order.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439eb67b-577b-4cf2-ac4c-5a3086206763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start to build image dataset from this csv order using the batches\n",
    "main_folder=batches[2]\n",
    "csv_file=csv_order[2]\n",
    "print(main_folder,csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sql connection to the table\n",
    "#apply filteration steps\n",
    "#then fetch one image and display\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "val_table=pd.read_csv(csv_file)\n",
    "\n",
    "print(val_table.shape)\n",
    "\n",
    "#freeze the base model\n",
    "def setting_up_labels(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return \"basic\"\n",
    "    if(\"overlap\" in label):\n",
    "        return \"overlap\"\n",
    "    if(\"proof\" in label):\n",
    "        return \"proof\"\n",
    "    if(\"theorem\" in label):\n",
    "        return  \"theorem\"\n",
    "    else:\n",
    "        return \"theorem\"\n",
    "    \n",
    "    \n",
    "def setting_up_labels_1(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return 0\n",
    "    if(\"overlap\" in label):\n",
    "        return 3\n",
    "    if(\"proof\" in label):\n",
    "        return 1\n",
    "    if(\"theorem\" in label):\n",
    "        return  2\n",
    "    \n",
    "val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels)\n",
    "val_table=val_table.dropna()\n",
    "val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs=list(val_table[\"pdf_path\"].unique())\n",
    "print(len(pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_table[\"page_no\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "val_table['page_no'] = val_table[\"page_no\"].apply(lambda x: literal_eval(str(x)))\n",
    "val_table['top_left'] = val_table[\"top_left\"].apply(lambda x: literal_eval(str(x)))\n",
    "val_table['bot_right'] = val_table[\"bot_right\"].apply(lambda x: literal_eval(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_table.shape) #has to be 6 columned\n",
    "\n",
    "print(val_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2f822-45be-4218-82d2-975cc432f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "def make_csv_for_extracting_images(csv_path):\n",
    "    val_table=pd.read_csv(csv_path)\n",
    "\n",
    "    #freeze the base model\n",
    "    def setting_up_labels(label):\n",
    "        label=label.lower()\n",
    "        if(\"basic\" in label):\n",
    "            return \"basic\"\n",
    "        if(\"overlap\" in label):\n",
    "            return \"overlap\"\n",
    "        if(\"proof\" in label):\n",
    "            return \"proof\"\n",
    "        if(\"theorem\" in label):\n",
    "            return  \"theorem\"\n",
    "        else:\n",
    "            return \"theorem\"\n",
    "\n",
    "\n",
    "    def setting_up_labels_1(label):\n",
    "        label=label.lower()\n",
    "        if(\"basic\" in label):\n",
    "            return 0\n",
    "        if(\"overlap\" in label):\n",
    "            return 3\n",
    "        if(\"proof\" in label):\n",
    "            return 1\n",
    "        if(\"theorem\" in label):\n",
    "            return  2\n",
    "\n",
    "    val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels)\n",
    "    val_table=val_table.dropna()\n",
    "    val_table[\"label\"]=val_table[\"label\"].apply(setting_up_labels_1)\n",
    "    \n",
    "\n",
    "    val_table['page_no'] = val_table[\"page_no\"].apply(lambda x: literal_eval(str(x)))\n",
    "    val_table['top_left'] = val_table[\"top_left\"].apply(lambda x: literal_eval(str(x)))\n",
    "    val_table['bot_right'] = val_table[\"bot_right\"].apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "    \n",
    "    return val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "modern-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_image(row,folder_name,image_location,ext=\".png\"):\n",
    "    try:\n",
    "        label=row[1][\"label\"]\n",
    "        local_folder=image_location+\"/\"+folder_name+\"/label_\"+str(label)\n",
    "        if(not os.path.exists(local_folder)):\n",
    "            os.mkdir(local_folder)\n",
    "        image_path=row[1][\"pdf_path\"].rsplit(\"/\",1)[0]\n",
    "        sub_folder=image_path.split(\".\")[0]\n",
    "        generic_path=image_path+\"/images\"+\"/image_{}.png\".format(str(row[1][\"page_no\"]))\n",
    "        file_name=generic_path\n",
    "        full_path=os.path.join(results_directory,file_name)\n",
    "        img_file_name=str(generic_path.rsplit(\"/\",2)[0])+\"_\"+str(row[1][\"page_no\"])+\"_\"+str(row[0])+ext\n",
    "        full_name=os.path.join(local_folder,img_file_name)\n",
    "        if(os.path.exists(full_name)):\n",
    "            return 1\n",
    "        img = cv2.imread(full_path) #add 0 for grey scale\n",
    "        top_left=math.floor(row[1][\"top_left\"][0]),math.floor(row[1][\"top_left\"][1])\n",
    "        bot_right=math.floor(row[1][\"bot_right\"][0]),math.floor(row[1][\"bot_right\"][1])\n",
    "        cropped_image = img[top_left[1]:bot_right[1], top_left[0]:bot_right[0]]\n",
    "        status=cv2.imwrite(full_name, cropped_image)\n",
    "    except TypeError or FileExistsError:\n",
    "        return\n",
    "    except:\n",
    "        print(\"error--\",row[1][\"pdf_path\"])\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e84efdf-bf65-4b2a-8a45-645316809e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfsscratch/rech/zpf/uyf36me/training_batches/batch_28\n",
      "/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_28.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(csv)\n\u001b[1;32m      6\u001b[0m results_directory\u001b[38;5;241m=\u001b[39mbatch\n\u001b[0;32m----> 7\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mmake_csv_for_extracting_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m pdfs_in_csv\u001b[38;5;241m=\u001b[39m[pdf\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()]\n\u001b[1;32m     10\u001b[0m pdfs_in_folder\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(batch)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mmake_csv_for_extracting_images\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_csv_for_extracting_images\u001b[39m(csv_path):\n\u001b[0;32m----> 5\u001b[0m     val_table\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#freeze the base model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetting_up_labels\u001b[39m(label):\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1253\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "for ind in range(0,100):\n",
    "    batch,csv=batches[ind],csv_order[ind]\n",
    "    print(batch)\n",
    "    print(csv)\n",
    "\n",
    "    results_directory=batch\n",
    "    df=make_csv_for_extracting_images(csv)\n",
    "\n",
    "    pdfs_in_csv=[pdf.split(\"/\")[0] for pdf in df[\"pdf_path\"].unique()]\n",
    "    pdfs_in_folder=os.listdir(batch)\n",
    "    print(pdfs_in_csv[0],pdfs_in_folder[0])\n",
    "\n",
    "    cnt=0\n",
    "    for pdf in pdfs_in_csv:\n",
    "        if(pdf in pdfs_in_folder):\n",
    "            cnt+=1\n",
    "    print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d54b17d-c6ed-4b1c-81fd-8634d4871fde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pdfs\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      4\u001b[0m     pdfs\u001b[38;5;241m.\u001b[39mappend(pdf\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m folders\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gpfsscratch/rech/zpf/uyf36me/training_batches/batch_7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "pdfs=[]\n",
    "\n",
    "for pdf in df[\"pdf_path\"].unique():\n",
    "    pdfs.append(pdf.split(\"/\")[0])\n",
    "    \n",
    "folders=os.listdir(\"/gpfsscratch/rech/zpf/uyf36me/training_batches/batch_7\")\n",
    "\n",
    "cnt=0\n",
    "for pdf in pdfs:\n",
    "    if(pdf not in folders):\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cca50825-95be-46ad-8415-759524cf41d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1106.2378', '1611.08464', '1104.4879', '1511.07907', '1409.6086']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29136fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.877717733383179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_location=\"/gpfsscratch/rech/zpf/uyf36me/training_patches\" #path to the dir where you will store the patches this should exist before\n",
    "folder_name=\"batch_7\"\n",
    "results_directory=\"/gpfsscratch/rech/zpf/uyf36me/training_batches/batch_7\" #the directory where you saved data csvs images etc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tick=time.time()\n",
    "\n",
    "val_table=make_csv_for_extracting_images(csv_order[2])\n",
    "for row in tqdm(val_table.iterrows()):\n",
    "    if(not os.path.exists(os.path.join(image_location,folder_name))):\n",
    "        os.mkdir(os.path.join(image_location,folder_name))\n",
    "    df=val_table\n",
    "    extract_and_save_image(row,folder_name,image_location,ext=\".png\")\n",
    "    break\n",
    "    \n",
    "tock=time.time()\n",
    "print(tock-tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-steering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/batch_7.csv\n",
      "(201518, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s][Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 39 concurrent workers.\n",
      "1it [00:00,  4.36it/s][Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-2)]: Done 287 tasks      | elapsed:    0.3s\n",
      "429it [00:00, 1618.89it/s][Parallel(n_jobs=-2)]: Done 570 tasks      | elapsed:    0.4s\n",
      "819it [00:00, 2413.06it/s][Parallel(n_jobs=-2)]: Done 935 tasks      | elapsed:    0.5s\n",
      "1209it [00:00, 2875.49it/s][Parallel(n_jobs=-2)]: Done 1380 tasks      | elapsed:    0.6s\n",
      "1875it [00:00, 3054.23it/s][Parallel(n_jobs=-2)]: Done 1907 tasks      | elapsed:    0.8s\n",
      "2548it [00:00, 3054.25it/s][Parallel(n_jobs=-2)]: Done 2514 tasks      | elapsed:    1.0s\n",
      "3167it [00:01, 2622.55it/s][Parallel(n_jobs=-2)]: Done 3203 tasks      | elapsed:    1.3s\n",
      "3744it [00:01, 2592.59it/s][Parallel(n_jobs=-2)]: Done 3972 tasks      | elapsed:    1.6s\n",
      "4676it [00:01, 2540.61it/s][Parallel(n_jobs=-2)]: Done 4823 tasks      | elapsed:    1.9s\n",
      "5609it [00:02, 2933.49it/s][Parallel(n_jobs=-2)]: Done 5754 tasks      | elapsed:    2.2s\n",
      "6831it [00:02, 2507.19it/s][Parallel(n_jobs=-2)]: Done 6767 tasks      | elapsed:    2.6s\n",
      "7683it [00:02, 2666.83it/s][Parallel(n_jobs=-2)]: Done 7860 tasks      | elapsed:    3.0s\n",
      "8739it [00:03, 3025.00it/s][Parallel(n_jobs=-2)]: Done 9035 tasks      | elapsed:    3.3s\n",
      "10315it [00:03, 2645.51it/s][Parallel(n_jobs=-2)]: Done 10290 tasks      | elapsed:    3.9s\n",
      "11622it [00:04, 2547.33it/s][Parallel(n_jobs=-2)]: Done 11627 tasks      | elapsed:    4.4s\n",
      "12793it [00:04, 2580.64it/s][Parallel(n_jobs=-2)]: Done 13044 tasks      | elapsed:    4.9s\n",
      "14274it [00:05, 3410.25it/s][Parallel(n_jobs=-2)]: Done 14543 tasks      | elapsed:    5.3s\n",
      "16185it [00:05, 2790.57it/s][Parallel(n_jobs=-2)]: Done 16122 tasks      | elapsed:    5.9s\n",
      "17745it [00:06, 2905.74it/s][Parallel(n_jobs=-2)]: Done 17783 tasks      | elapsed:    6.5s\n",
      "19357it [00:07, 2760.71it/s][Parallel(n_jobs=-2)]: Done 19524 tasks      | elapsed:    7.1s\n",
      "21216it [00:07, 2296.64it/s][Parallel(n_jobs=-2)]: Done 21347 tasks      | elapsed:    7.9s\n",
      "23239it [00:08, 2329.97it/s][Parallel(n_jobs=-2)]: Done 23250 tasks      | elapsed:    8.7s\n",
      "25158it [00:09, 2323.24it/s][Parallel(n_jobs=-2)]: Done 25235 tasks      | elapsed:    9.5s\n",
      "27300it [00:10, 2603.29it/s][Parallel(n_jobs=-2)]: Done 27300 tasks      | elapsed:   10.3s\n",
      "29454it [00:11, 2208.78it/s][Parallel(n_jobs=-2)]: Done 29447 tasks      | elapsed:   11.4s\n",
      "31707it [00:12, 2741.30it/s][Parallel(n_jobs=-2)]: Done 31674 tasks      | elapsed:   12.3s\n",
      "33852it [00:13, 2486.60it/s][Parallel(n_jobs=-2)]: Done 33983 tasks      | elapsed:   13.2s\n",
      "36309it [00:14, 2560.56it/s][Parallel(n_jobs=-2)]: Done 36372 tasks      | elapsed:   14.2s\n",
      "38883it [00:15, 2323.81it/s][Parallel(n_jobs=-2)]: Done 38843 tasks      | elapsed:   15.3s\n",
      "41388it [00:16, 2344.83it/s][Parallel(n_jobs=-2)]: Done 41394 tasks      | elapsed:   16.5s\n",
      "43924it [00:17, 2379.27it/s][Parallel(n_jobs=-2)]: Done 44027 tasks      | elapsed:   17.6s\n",
      "46609it [00:18, 2206.93it/s][Parallel(n_jobs=-2)]: Done 46740 tasks      | elapsed:   18.8s\n",
      "49491it [00:19, 2382.74it/s][Parallel(n_jobs=-2)]: Done 49535 tasks      | elapsed:   20.0s\n",
      "52377it [00:20, 3025.99it/s][Parallel(n_jobs=-2)]: Done 52410 tasks      | elapsed:   21.0s\n",
      "55263it [00:22, 2745.12it/s][Parallel(n_jobs=-2)]: Done 55367 tasks      | elapsed:   22.2s\n",
      "58227it [00:23, 2775.55it/s][Parallel(n_jobs=-2)]: Done 58404 tasks      | elapsed:   23.3s\n",
      "61347it [00:24, 2622.09it/s][Parallel(n_jobs=-2)]: Done 61523 tasks      | elapsed:   24.6s\n",
      "64654it [00:26, 2116.02it/s][Parallel(n_jobs=-2)]: Done 64722 tasks      | elapsed:   26.2s\n",
      "67977it [00:27, 2682.16it/s][Parallel(n_jobs=-2)]: Done 68003 tasks      | elapsed:   27.5s\n",
      "71409it [00:29, 2370.78it/s][Parallel(n_jobs=-2)]: Done 71364 tasks      | elapsed:   29.1s\n",
      "74669it [00:30, 2549.66it/s][Parallel(n_jobs=-2)]: Done 74807 tasks      | elapsed:   30.5s\n",
      "78273it [00:31, 2428.52it/s][Parallel(n_jobs=-2)]: Done 78330 tasks      | elapsed:   32.0s\n",
      "81808it [00:33, 2286.39it/s][Parallel(n_jobs=-2)]: Done 81935 tasks      | elapsed:   33.6s\n",
      "85504it [00:35, 2476.71it/s][Parallel(n_jobs=-2)]: Done 85620 tasks      | elapsed:   35.1s\n",
      "89399it [00:36, 2363.32it/s][Parallel(n_jobs=-2)]: Done 89387 tasks      | elapsed:   36.6s\n",
      "93249it [00:38, 2807.70it/s][Parallel(n_jobs=-2)]: Done 93234 tasks      | elapsed:   38.4s\n",
      "97238it [00:40, 2156.18it/s][Parallel(n_jobs=-2)]: Done 97163 tasks      | elapsed:   40.1s\n",
      "101127it [00:41, 2025.46it/s][Parallel(n_jobs=-2)]: Done 101172 tasks      | elapsed:   41.9s\n",
      "105362it [00:43, 2065.89it/s][Parallel(n_jobs=-2)]: Done 105263 tasks      | elapsed:   43.8s\n",
      "106813it [01:04, 34.59it/s]  "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_location=\"/gpfsscratch/rech/zpf/uyf36me/training_patches\"\n",
    "results_directory=\"/gpfsscratch/rech/zpf/uyf36me/training_batches/batch_7\" #the directory where you saved data csvs images etc\n",
    "n_jobs=-2\n",
    "\n",
    "print(csv_order[2])\n",
    "df=make_csv_for_extracting_images(csv_order[2])\n",
    "print(df.shape)\n",
    "\n",
    "for df,folder_name in zip([df],[\"batch_7\"]):\n",
    "    if(not os.path.exists(os.path.join(image_location,folder_name))):\n",
    "        os.mkdir(os.path.join(image_location,folder_name))\n",
    "    res=Parallel(n_jobs=n_jobs,backend=\"threading\",verbose=2)(delayed(extract_and_save_image)\n",
    "                                           (row,folder_name,image_location) for row in tqdm(df.iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82a529ba-c9ab-4bd8-b7a6-484d02b5ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "results_directory=\"/gpfsscratch/rech/zpf/uyf36me/training_patches/batch_2/**/**.png\"\n",
    "files=glob.glob(results_directory)\n",
    "unique=set([file.split(\"/\")[-1].split(\"_\")[0] for file in files])\n",
    "#print(unique)\n",
    "print(len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969404e6-142c-4e1f-825b-d98134752ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199146, 26)\n",
      "----doing batch batch_114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s][Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 39 concurrent workers.\n",
      "178it [00:10,  4.99it/s][Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed:   10.4s\n",
      "351it [00:21, 28.73it/s][Parallel(n_jobs=-2)]: Done 287 tasks      | elapsed:   21.5s\n",
      "624it [00:39, 28.04it/s][Parallel(n_jobs=-2)]: Done 570 tasks      | elapsed:   39.4s\n",
      "984it [01:03,  8.31it/s][Parallel(n_jobs=-2)]: Done 935 tasks      | elapsed:  1.1min\n",
      "1421it [01:32, 11.91it/s][Parallel(n_jobs=-2)]: Done 1380 tasks      | elapsed:  1.5min\n",
      "1958it [02:07, 12.99it/s][Parallel(n_jobs=-2)]: Done 1907 tasks      | elapsed:  2.1min\n",
      "2338it [02:31, 27.66it/s]Exception in thread Thread-47 (_handle_results):\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/multiprocessing/pool.py\", line 592, in _handle_results\n",
      "    cache[job]._set(i, obj)\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/multiprocessing/pool.py\", line 776, in _set\n",
      "    self._callback(self._value)\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/joblib/parallel.py\", line 359, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/joblib/parallel.py\", line 794, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 252, in apply_async\n",
      "    return self._get_pool().apply_async(\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/multiprocessing/pool.py\", line 455, in apply_async\n",
      "    self._check_running()\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/tensorflow-2.9.1+py3.10/lib/python3.10/multiprocessing/pool.py\", line 350, in _check_running\n",
      "    raise ValueError(\"Pool not running\")\n",
      "ValueError: Pool not running\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "2338it [02:46, 27.66it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_location=\"/gpfsscratch/rech/zpf/uyf36me/training_patches\"\n",
    "\n",
    "n_jobs=-2\n",
    "ind=10\n",
    "\n",
    "batch,csv=batches[ind],csv_order[ind]\n",
    "results_directory=batch\n",
    "df=make_csv_for_extracting_images(csv)\n",
    "print(df.shape)\n",
    "folder_name=batch.split(\"/\")[-1]\n",
    "print(\"----doing batch\", folder_name)\n",
    "if(not os.path.exists(os.path.join(image_location,folder_name))):\n",
    "    os.mkdir(os.path.join(image_location,folder_name))\n",
    "res=Parallel(n_jobs=n_jobs,backend=\"threading\",verbose=2)(delayed(extract_and_save_image)\n",
    "                                       (row,folder_name,image_location) for row in tqdm(df.iterrows()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64963048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "#freeze the base model\n",
    "def setting_up_labels(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return \"basic\"\n",
    "    if(\"overlap\" in label):\n",
    "        return \"overlap\"\n",
    "    if(\"proof\" in label):\n",
    "        return \"proof\"\n",
    "    if(\"theorem\" in label):\n",
    "        return  \"theorem\"\n",
    "    \n",
    "def setting_up_labels_1(label):\n",
    "    label=label.lower()\n",
    "    if(\"basic\" in label):\n",
    "        return 0\n",
    "    if(\"overlap\" in label):\n",
    "        return 3\n",
    "    if(\"proof\" in label):\n",
    "        return 1\n",
    "    if(\"theorem\" in label):\n",
    "        return  2\n",
    "    \n",
    "\n",
    "def preprocess_df_for_vision(df):\n",
    "    #string conversion\n",
    "    df['page_no'] = df[\"page_no\"].apply(lambda x: literal_eval(str(x)))\n",
    "    df['top_left'] = df[\"top_left\"].apply(lambda x: literal_eval(str(x)))\n",
    "    df['bot_right'] = df[\"bot_right\"].apply(lambda x: literal_eval(str(x)))\n",
    "    df[\"label\"]=df[\"label\"].apply(setting_up_labels)\n",
    "    #dropping nans same as in NLP approach\n",
    "    df=df.dropna()\n",
    "    df[\"label\"]=df[\"label\"].apply(setting_up_labels_1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#order of execution\n",
    "names=['batch_28.csv',\n",
    " 'batch_18.csv',\n",
    " 'batch_108.csv',\n",
    " 'batch_106.csv',\n",
    " 'batch_85.csv',\n",
    " 'batch_123.csv',\n",
    " 'batch_7.csv',\n",
    " 'batch_48.csv',\n",
    " 'batch_2.csv',\n",
    " 'batch_114.csv',\n",
    " 'batch_125.csv',\n",
    " 'batch_67.csv',\n",
    " 'batch_4.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "structural-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "image_location=\"/Users/mv96/Downloads/vision_patches\"\n",
    "results_directory=\"/Volumes/My_Book/Theoremkb/cluster_package/5-grobid_on_pdfs/ftest\" #the directory where you saved data csvs images etc\n",
    "n_jobs=-2\n",
    "\n",
    "train_data_dir=\"/Volumes/My_Book/Theoremkb/cluster_package/8-finetuning_data/train_data\"\n",
    "csv_files=glob.glob(train_data_dir+\"/**.csv\")\n",
    "\n",
    "order=[]\n",
    "for name in names:\n",
    "    for file in csv_files:\n",
    "        if(name in file):\n",
    "            order.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee29c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(order):\n",
    "    folder_name=file[:-4].rsplit(\"/\")[-1]\n",
    "    df=pd.read_csv(file)\n",
    "    df=preprocess_df_for_vision(df)\n",
    "    print(folder_name,df.shape)\n",
    "    \n",
    "    #continue if the zip file or folder they exist\n",
    "    zip_path=os.path.join(image_location,folder_name)+\".zip\"\n",
    "    if(os.path.exists(zip_path)):\n",
    "        continue\n",
    "    if(not os.path.exists(os.path.join(image_location,folder_name))):\n",
    "        os.mkdir(os.path.join(image_location,folder_name))\n",
    "    else:\n",
    "        continue\n",
    "    res=Parallel(n_jobs=n_jobs,backend=\"threading\",verbose=2)(delayed(extract_and_save_image)\n",
    "                                           (row,df,folder_name,image_location) for row in tqdm(df.iterrows()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absent-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/MV96/vision_patches/test_data\n",
      "76986\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_location=\"/Volumes/MV96/vision_patches\"\n",
    "check_dir=os.path.join(image_location,\"test_data\")\n",
    "print(check_dir)\n",
    "counts=0\n",
    "for label in os.listdir(check_dir):\n",
    "    counts+=len(os.listdir(os.path.join(check_dir,label)))\n",
    "    #print(train_table.shape)\n",
    "print(counts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "intellectual-context",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17511"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-carroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.1_py3.10",
   "language": "python",
   "name": "module-conda-env-tensorflow-2.9.1_py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
