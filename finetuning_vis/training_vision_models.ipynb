{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b070f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 08:48:45.396727: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 08:48:45.642177: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strategy=tf.distribute.MirroredStrategy()\n",
    "devices=strategy.num_replicas_in_sync\n",
    "\n",
    "print(\"no of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point the dataset has been created and now we will serve it to the model\n",
    "#but since the data can't be loaded directly into the memory\n",
    "#we will use tensorflow dataset to efficiently load the data in batches\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow_addons.optimizers import AdamW ,LAMB\n",
    "import cv2\n",
    "import zipfile\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "from focal_loss import SparseCategoricalFocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_classifier_on_zipped_data:\n",
    "    def __init__(self,names,path_to_extract=None,zip_direc=None,\n",
    "                 batch_per_gpu=16,\n",
    "                image_shapes=(400,1400)):\n",
    "        \n",
    "        # constructor\n",
    "        strategy=tf.distribute.MirroredStrategy()\n",
    "        devices=strategy.num_replicas_in_sync\n",
    "        self.batch_per_gpu=batch_per_gpu\n",
    "        self.image_shapes=(400,1400)\n",
    "        self.input_shape=(None,self.image_shapes[0],self.image_shapes[1],3)\n",
    "        self.batch_size=self.batch_per_gpu*devices\n",
    "        self.input_shape=(None,self.image_shapes[0],self.image_shapes[1],3)\n",
    "        if(path_to_extract==None):\n",
    "            self.path_to_extract=zip_direc #destination of output path\n",
    "        if(zip_direc is not None):\n",
    "            self.zip_direc=os.path.join(zip_direc,\"**.zip\")\n",
    "        else:\n",
    "            self.zip_direc=zip_direc\n",
    "        self.names=names #order in which we want to pass the data\n",
    "        \n",
    "        \n",
    "        \n",
    "    def white_padding_and_scaling(self,default_shape,file_loc,overwrite=False):\n",
    "        \"\"\"\n",
    "        2- adds white padding wherever necessary\n",
    "        3- takes bitwise NOT transformation this esentially inverts the image sets black -0 as background while \n",
    "        255 is set as foreground\n",
    "        4- if overwrite true then makes a new file with '_t' suffix \n",
    "        \"\"\"\n",
    "        try:\n",
    "            img_array=cv2.imread(file_loc)\n",
    "            shape=img_array.shape\n",
    "        except:\n",
    "            print(\"error in white padding--\",file_loc)\n",
    "            print(\"hence removing the file\",file_loc)\n",
    "            os.remove(file_loc)\n",
    "            return\n",
    "        \n",
    "        padding_height=0\n",
    "        padding_width=0\n",
    "        crop_width=False\n",
    "        crop_height=False\n",
    "\n",
    "        if(shape[0]<=default_shape[0]): #if img is small in width then we need padding then \n",
    "            padding_height=default_shape[0]-shape[0]\n",
    "        else:\n",
    "            crop_height=True\n",
    "            padding_height=0\n",
    "        if(shape[1]<=default_shape[1]):\n",
    "            padding_width=default_shape[1]-shape[1]\n",
    "        else:\n",
    "            crop_width=True\n",
    "            padding_width=0\n",
    "        if(padding_width>0 or padding_height>0):\n",
    "            colour_fill=(255,255,255) #colour to pad this is white\n",
    "            new_array=cv2.copyMakeBorder(img_array, 0,padding_height , 0, padding_width, cv2.BORDER_CONSTANT,value=colour_fill)\n",
    "        else:\n",
    "            new_array=img_array[0:default_shape[0], 0:default_shape[1]]\n",
    "\n",
    "        if(crop_width==True):\n",
    "            new_array=new_array[0:default_shape[0], 0:default_shape[1]]\n",
    "        if(crop_height==True):\n",
    "            new_array=new_array[0:default_shape[0], 0:default_shape[1]]\n",
    "\n",
    "\n",
    "        new_array=cv2.bitwise_not(new_array)\n",
    "        if(overwrite==True):\n",
    "            new_name=file_loc.replace(\".png\",\"_t.png\")\n",
    "            #print(new_name)\n",
    "            cv2.imwrite(new_name,new_array)\n",
    "            os.remove(file_loc)\n",
    "            return\n",
    "\n",
    "        return new_array\n",
    "    \n",
    "    def append_last_run(self,val,log_file):\n",
    "        with open(log_file,\"a\") as f:\n",
    "            f.write(val)\n",
    "            \n",
    "    def get_last_run(self):\n",
    "        with open(self.log_file,\"r\") as f:\n",
    "            lines=f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        files=[]\n",
    "        for line in lines:\n",
    "            files.append(line.split(\",\")[0])\n",
    "\n",
    "        return files\n",
    "    \n",
    "    def calc_class_weigths(self,_dir):\n",
    "        sub_dir=list(filter(lambda x: x.startswith(\"label\") ,os.listdir(_dir)))\n",
    "        sub_dir.sort()\n",
    "        class_names=[]\n",
    "        for folder in sub_dir:\n",
    "            glob_path=os.path.join(_dir,folder)+\"/**_t.png\"\n",
    "            files=glob.glob(glob_path)\n",
    "            class_names.append(len(files))\n",
    "\n",
    "        class_weights=np.array(class_names)\n",
    "        weights=np.full(shape=len(class_weights),fill_value=sum(class_weights))\n",
    "        out = np.divide(weights, class_weights)\n",
    "\n",
    "        class_weights=np.around(out)\n",
    "        class_weight_dict={}\n",
    "        for i,weight in enumerate(class_weights):\n",
    "            class_weight_dict[i]=weight\n",
    "\n",
    "\n",
    "        return class_weight_dict\n",
    "    \n",
    "    def fit(self,model,model_path=None,validation_data=None,n_jobs=-2,class_weight=None,log_file=\"my_model.txt\"):\n",
    "        \n",
    "        order=[]\n",
    "        names=self.names\n",
    "        \n",
    "        if(self.zip_direc is not None):\n",
    "            print(zip_direc)\n",
    "            zip_files=glob.glob(self.zip_direc)\n",
    "            self.log_file=log_file\n",
    "\n",
    "            #building the right order of execution\n",
    "\n",
    "            for zip_file in zip_files:\n",
    "                chk_val=zip_file[:-4].split(\"/\")[-1]\n",
    "                if(chk_val in names):\n",
    "                    order.append(zip_file)\n",
    "        \n",
    "\n",
    "        for name in names:\n",
    "            full_name=os.path.join(os.getcwd(),name)\n",
    "            if(full_name+\".zip\" not in order):\n",
    "                order.append(full_name)\n",
    "                \n",
    "                \n",
    "        i=1\n",
    "        for zip_file in order: #take a batched zip file unzip it in $SCRATCH dir\n",
    "            self.log_file=log_file\n",
    "            \n",
    "            try:\n",
    "                run_so_far=self.get_last_run()\n",
    "                if(not zip_file.endswith(\".zip\")):\n",
    "                    zip_file+=\".zip\"\n",
    "                    \n",
    "                if(zip_file[:-4] in run_so_far): #or file in skips\n",
    "                    print(\"already ran on:\",zip_file[:-4])\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"not in previous runs, hence running on this batch\")\n",
    "                    print(zip_file)\n",
    "            except FileNotFoundError:\n",
    "                print(\"check for \",log_file)\n",
    "                pass           \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print(\"--unzipping file--\")\n",
    "            \n",
    "            #skip if unzipped folder is there\n",
    "            skip_dir=False\n",
    "            if(os.path.isdir(zip_file)):\n",
    "                zip_file=zip_file+\".zip\"\n",
    "                skip_dir=True\n",
    "                \n",
    "            if(not os.path.exists(zip_file[:-4]) and skip_dir==False):\n",
    "                with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "                    print(f\"extracting {zip_file} at--\",self.path_to_extract)\n",
    "                    zip_ref.extractall(self.path_to_extract)\n",
    "\n",
    "            \n",
    "            #we need to transform these images \n",
    "            path_chk=zip_file[:-4]+\"/**/**.png\"\n",
    "            png_files=glob.glob(path_chk)\n",
    "            \n",
    "            \n",
    "            filtered_files=list(filter(lambda x: not x.endswith(\"_t.png\"),png_files))\n",
    "            \n",
    "            bad_files=list(filter(lambda x:  x.endswith(\"_t_t.png\"),png_files))\n",
    "            if(len(bad_files)!=0):\n",
    "                print(\"many reruns please check the data again\")\n",
    "                return \n",
    "            \n",
    "            \n",
    "            \n",
    "            print(\"--running transformations\")\n",
    "            print(f\"len of files to run in{zip_file[:-4]} :{len(filtered_files)}\")\n",
    "            \n",
    "\n",
    "            res=Parallel(n_jobs=n_jobs,backend=\"threading\",verbose=2)(delayed(self.white_padding_and_scaling)\n",
    "                                                       (default_shape=image_shapes,file_loc=fname,overwrite=True) for fname in tqdm(filtered_files))\n",
    "            \n",
    "            #converting it in the tf dataset object\n",
    "            print(\"--converting it in tf dataset object\")\n",
    "            if(class_weight==\"balanced\"):\n",
    "                class_weight_dict=self.calc_class_weigths(zip_file[:-4])\n",
    "                \n",
    "            \n",
    "            train_dataset_batch=tf.keras.preprocessing.image_dataset_from_directory(directory=zip_file[:-4],\n",
    "                                                                                    image_size=image_shapes,\n",
    "                                                                                    batch_size=batch_size,\n",
    "                                                                                    seed=2,\n",
    "                                                                                    shuffle=True)\n",
    "            \n",
    "            train_dataset_batch=train_dataset_batch.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "            if(validation_data!=None):\n",
    "                validation_data=validation_data.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "            with strategy.scope():\n",
    "                if(model_path is not None and os.path.exists(model_path)):\n",
    "                    print(f\"model exists in{model_path}\")\n",
    "                    ##change this <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< uncomment\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(\"=\"*20)\n",
    "                    print(model.summary())\n",
    "                    print(\"=\"*20)\n",
    "                else:\n",
    "                    model_path=log_file.replace(\".txt\",\".h5\")\n",
    "                    \n",
    "                \n",
    "                if(class_weight is None):\n",
    "                    history = model.fit(train_dataset_batch, epochs=1,validation_data=validation_data)\n",
    "                    pass\n",
    "                elif(class_weight==\"balanced\"):\n",
    "                    print(f\"class_weight is balanced{class_weight_dict}\")\n",
    "                    history = model.fit(train_dataset_batch,class_weight=class_weight_dict,epochs=1)\n",
    "                    #val_loss,val_acc=model.evaluate(validation_data)\n",
    "                \n",
    "                \n",
    "                #metrics after a batch\n",
    "                train_loss=history.history['loss']\n",
    "                train_acc=history.history['accuracy']\n",
    "                \n",
    "                if(validation_data is not None):\n",
    "                    val_loss=history.history['val_loss']\n",
    "                    val_acc=history.history['val_accuracy']\n",
    "                else:\n",
    "                    val_loss=\"NA\"\n",
    "                    val_acc=\"NA\"\n",
    "                \n",
    "\n",
    "                line_to_write=f'{zip_file[:-4]},{train_loss},{val_loss},{train_acc},{val_acc}\\n'\n",
    "                self.append_last_run(line_to_write,log_file)\n",
    "                \n",
    "                #saving the model\n",
    "                save_model=model_path[:-3]+str(i)+\".h5\"\n",
    "                #print(save_model)\n",
    "                model.save(save_model)\n",
    "                i+=1\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e535b-c03e-4496-8133-f9d4ff6f03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "######loading small validation data incase we are not abe to upload full validation data\n",
    "image_shapes=(400,1400)\n",
    "batch_per_gpu=4 #####<<<<<<<<<<<<<<<<<<<<<<<<<<<change this to fit in gpu\n",
    "batch_size=batch_per_gpu*devices\n",
    "\n",
    "sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"/gpfsscratch/rech/zpf/uyf36me/validation_patches\",\n",
    "    image_size=image_shapes,\n",
    "    batch_size=batch_size,\n",
    "    seed=2,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604d0f0-a281-4923-aa79-f19d8b85cecd",
   "metadata": {},
   "source": [
    "######loading small validation data incase we are not abe to upload full validation data\n",
    "image_shapes=(400,1400)\n",
    "batch_per_gpu=4 #####<<<<<<<<<<<<<<<<<<<<<<<<<<<change this to fit in gpu\n",
    "batch_size=batch_per_gpu*devices\n",
    "\n",
    "full_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/val_data\",\n",
    "    image_size=image_shapes,\n",
    "    batch_size=batch_size,\n",
    "    seed=2,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "full_validation_dataset=full_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba0a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordering file name accroding to some predefined run (which is here nlp)\n",
    "train_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/train_data/**.csv\"\n",
    "import glob\n",
    "files=glob.glob(train_path)\n",
    "file_path=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/start_mlm_forft_res.txt\"\n",
    "import os\n",
    "base=\"/linkhome/rech/gennsp01/uyf36me/work/finetuning_NLP/resources/\"\n",
    "with open(file_path,\"r\") as f:\n",
    "    lines=f.readlines()\n",
    "start=[]\n",
    "for line in lines:\n",
    "    val=line.split(\",\")[0][2:]\n",
    "    start.append(val[:-4].rsplit(\"/\")[1])\n",
    "\n",
    "#get the order of execution of batch        \n",
    "order=start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64614857-132a-405b-b24b-2b8dd3518c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e417cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pngs=glob.glob(\"/gpfsscratch/rech/zpf/uyf36me/training_patches/batch_2/**/**.png\")\n",
    "print(len(pngs))\n",
    "filtered_files=list(filter(lambda x: not x.endswith(\"_t.png\"),pngs))\n",
    "len(filtered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a108ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 400, 1400, 3)      0         \n",
      "                                                                 \n",
      " efficientnetv2-m (Functiona  (None, 1280)             53150388  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 5124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,155,512\n",
      "Trainable params: 52,863,480\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_28\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_18\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_7\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_2\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_0\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_108\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_106\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_85\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_123\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_48\n",
      "already ran on: /gpfsscratch/rech/zpf/uyf36me/training_patches/batch_114\n",
      "not in previous runs, hence running on this batch\n",
      "/gpfsscratch/rech/zpf/uyf36me/training_patches/batch_125.zip\n",
      "--unzipping file--\n",
      "--running transformations\n",
      "len of files to run in/gpfsscratch/rech/zpf/uyf36me/training_patches/batch_125 :40153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40153 [00:00<?, ?it/s][Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:    0.1s\n",
      "  0%|          | 99/40153 [00:00<01:26, 461.01it/s][Parallel(n_jobs=-2)]: Done 140 tasks      | elapsed:    0.3s\n",
      "  1%|          | 341/40153 [00:00<01:18, 507.51it/s][Parallel(n_jobs=-2)]: Done 343 tasks      | elapsed:    0.7s\n",
      "  2%|▏         | 638/40153 [00:01<01:15, 524.62it/s][Parallel(n_jobs=-2)]: Done 626 tasks      | elapsed:    1.3s\n",
      "  2%|▏         | 1001/40153 [00:01<01:13, 535.30it/s][Parallel(n_jobs=-2)]: Done 991 tasks      | elapsed:    1.9s\n",
      "  4%|▎         | 1441/40153 [00:02<01:15, 511.53it/s][Parallel(n_jobs=-2)]: Done 1436 tasks      | elapsed:    2.8s\n",
      "  5%|▍         | 1973/40153 [00:03<01:42, 373.41it/s][Parallel(n_jobs=-2)]: Done 1963 tasks      | elapsed:    4.0s\n",
      "  6%|▋         | 2519/40153 [00:04<00:57, 659.26it/s][Parallel(n_jobs=-2)]: Done 2570 tasks      | elapsed:    5.0s\n",
      "  8%|▊         | 3256/40153 [00:06<01:00, 606.07it/s][Parallel(n_jobs=-2)]: Done 3259 tasks      | elapsed:    6.1s\n",
      " 10%|▉         | 3982/40153 [00:07<00:55, 651.07it/s][Parallel(n_jobs=-2)]: Done 4028 tasks      | elapsed:    7.7s\n",
      " 12%|█▏        | 4895/40153 [00:08<00:48, 721.98it/s][Parallel(n_jobs=-2)]: Done 4879 tasks      | elapsed:    8.9s\n",
      " 14%|█▍        | 5798/40153 [00:10<01:16, 449.53it/s][Parallel(n_jobs=-2)]: Done 5810 tasks      | elapsed:   10.5s\n",
      " 17%|█▋        | 6842/40153 [00:12<01:15, 440.11it/s][Parallel(n_jobs=-2)]: Done 6823 tasks      | elapsed:   12.8s\n",
      " 20%|█▉        | 7887/40153 [00:15<01:16, 420.88it/s][Parallel(n_jobs=-2)]: Done 7916 tasks      | elapsed:   15.5s\n",
      " 23%|██▎       | 9064/40153 [00:18<01:08, 453.28it/s][Parallel(n_jobs=-2)]: Done 9091 tasks      | elapsed:   18.2s\n",
      " 26%|██▌       | 10362/40153 [00:21<01:06, 448.62it/s][Parallel(n_jobs=-2)]: Done 10346 tasks      | elapsed:   21.0s\n",
      " 29%|██▉       | 11686/40153 [00:23<01:03, 447.66it/s][Parallel(n_jobs=-2)]: Done 11683 tasks      | elapsed:   24.0s\n",
      " 33%|███▎      | 13101/40153 [00:27<00:58, 462.71it/s][Parallel(n_jobs=-2)]: Done 13100 tasks      | elapsed:   27.2s\n",
      " 36%|███▋      | 14597/40153 [00:34<01:38, 259.29it/s][Parallel(n_jobs=-2)]: Done 14599 tasks      | elapsed:   34.7s\n",
      " 40%|████      | 16181/40153 [00:38<00:56, 425.01it/s][Parallel(n_jobs=-2)]: Done 16178 tasks      | elapsed:   38.8s\n",
      " 44%|████▍     | 17853/40153 [00:42<00:42, 526.61it/s][Parallel(n_jobs=-2)]: Done 17839 tasks      | elapsed:   42.9s\n",
      " 49%|████▊     | 19547/40153 [00:47<00:48, 429.05it/s][Parallel(n_jobs=-2)]: Done 19580 tasks      | elapsed:   47.2s\n",
      " 53%|█████▎    | 21384/40153 [00:50<00:31, 593.20it/s][Parallel(n_jobs=-2)]: Done 21403 tasks      | elapsed:   50.7s\n",
      " 58%|█████▊    | 23309/40153 [00:54<00:29, 578.20it/s][Parallel(n_jobs=-2)]: Done 23306 tasks      | elapsed:   54.9s\n",
      " 63%|██████▎   | 25256/40153 [00:58<00:23, 628.42it/s][Parallel(n_jobs=-2)]: Done 25291 tasks      | elapsed:   58.5s\n",
      " 68%|██████▊   | 27357/40153 [01:02<00:21, 592.78it/s][Parallel(n_jobs=-2)]: Done 27356 tasks      | elapsed:  1.0min\n",
      " 74%|███████▎  | 29519/40153 [01:07<00:21, 495.99it/s][Parallel(n_jobs=-2)]: Done 29503 tasks      | elapsed:  1.1min\n",
      " 79%|███████▉  | 31691/40153 [01:11<00:18, 455.29it/s][Parallel(n_jobs=-2)]: Done 31730 tasks      | elapsed:  1.2min\n",
      " 85%|████████▍ | 34039/40153 [01:15<00:13, 466.71it/s][Parallel(n_jobs=-2)]: Done 34039 tasks      | elapsed:  1.3min\n",
      " 91%|█████████ | 36421/40153 [01:21<00:08, 446.40it/s][Parallel(n_jobs=-2)]: Done 36428 tasks      | elapsed:  1.4min\n",
      " 97%|█████████▋| 38925/40153 [01:26<00:02, 489.24it/s][Parallel(n_jobs=-2)]: Done 38899 tasks      | elapsed:  1.4min\n",
      "100%|██████████| 40153/40153 [01:28<00:00, 454.14it/s]\n",
      "[Parallel(n_jobs=-2)]: Done 40153 out of 40153 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--converting it in tf dataset object\n",
      "Found 41470 files belonging to 4 classes.\n",
      "model exists in./new_models/r_efficientnetv2m_avg9.h5\n",
      "====================\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 400, 1400, 3)      0         \n",
      "                                                                 \n",
      " efficientnetv2-m (Functiona  (None, 1280)             53150388  \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 5124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,155,512\n",
      "Trainable params: 52,863,480\n",
      "Non-trainable params: 292,032\n",
      "_________________________________________________________________\n",
      "None\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 08:45:38.568277: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 41470\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:8\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 649 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    }
   ],
   "source": [
    "rescaling_layer=tf.keras.layers.Rescaling(scale=1./255) #we will use this later\n",
    "\n",
    "class efficientnetv2m:  #<<<<<<<<<<<<<<change this\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None,pooling=None):\n",
    "        #<<<<<<<<<<<<<<change this\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        #<<<<<<<<<<<<<<change this\n",
    "        base_model=tf.keras.applications.efficientnet_v2.EfficientNetV2M(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling=\"avg\") #<<<<<\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        if(pooling==None):\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "image_shapes=(400,1400)\n",
    "input_shape=(None,image_shapes[0],image_shapes[1],3)\n",
    "NUM_EPOCHS=1\n",
    "learning_rate=2e-5\n",
    "classes=4\n",
    "zip_direc=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches\"\n",
    "opt=LAMB(learning_rate)\n",
    "#opt=AdamW(learning_rate) #for focal loss only\n",
    "#opt=tf.keras.optimizers.Adam(learning_rate) #trying for foc loss\n",
    "\n",
    "with strategy.scope():\n",
    "    #<<<<<<<<<<<<<<<<<<<<<<change this\n",
    "    model = efficientnetv2m.build(input_shape=input_shape,\n",
    "                               classes=classes) \n",
    "\n",
    "\n",
    "    #loss is sparse because we have int features\n",
    "    loss_focal=tfa.losses.SigmoidFocalCrossEntropy() #only for binary classification\n",
    "    \n",
    "    default_loss=\"sparse_categorical_crossentropy\"\n",
    "    #code https://github.com/artemmavrin/focal-loss \n",
    "    #paper Focal Loss for Dense Object Detection\n",
    "    #loss_focal=SparseCategoricalFocalLoss(gamma=2)\n",
    "    model.compile(loss=default_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.build(input_shape=input_shape)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"efficientnetb7\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "\n",
    "\n",
    "#opt=tf.keras.optimizers.Adam()\n",
    "\n",
    "#if any batch not present then it will be skipped\n",
    "location_batches=\"/gpfsscratch/rech/zpf/uyf36me/training_patches\"\n",
    "batches=os.listdir(location_batches)\n",
    "names=[os.path.join(location_batches,batch) for batch in batches if batch.startswith(\"batch\")]\n",
    "#we need to reorder batches now\n",
    "order=['batch_28', 'batch_18', 'batch_7', 'batch_2', 'batch_0', 'batch_108', 'batch_106', 'batch_85',\n",
    "       'batch_123', 'batch_48', 'batch_114', 'batch_125', 'batch_67', 'batch_4', 'batch_93', 'batch_3',\n",
    "       'batch_86', 'batch_36', 'batch_41']\n",
    "\n",
    "\n",
    "new_names=[]\n",
    "for batch in order:\n",
    "    for name in names:\n",
    "        if(name.endswith(batch)):\n",
    "            new_names.append(name)\n",
    "            \n",
    "\n",
    "names=new_names\n",
    "\n",
    "clf=Image_classifier_on_zipped_data(names,batch_per_gpu=batch_per_gpu)\n",
    "\n",
    "#, validation_data=sub_sample_validation_dataset\n",
    "\n",
    "#provide model or model_path\n",
    "#<<<<<<<<<<<<<<<<<<<<<<change this\n",
    "files=clf.fit(model=model,model_path=\"./new_models/r_efficientnetv2m_avg9.h5\",\n",
    "        log_file=\"./new_models/r_efficientnetv2m_avg.txt\",class_weight=None) #validation_data=sub_sample_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d428f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#70.2 app for focal loss ep 1 lamb optmizer\n",
    "\n",
    "#234/2680 [=>............................] - ETA: 40:05 - loss: 1.2880 - accuracy: 0.5186 pool avg #20 mill\n",
    "#291/2680 [==>...........................] - ETA: 38:28 - loss: 0.8468 - accuracy: 0.6633 no pool  #23 mill\n",
    "#235/2680 [=>............................] - ETA: 39:27 - loss: 4.8293 - accuracy: 0.3949 pool max #20 mill\n",
    "\n",
    "#hence we choose no pooling\n",
    "\n",
    "#around 20000 steps of 16 bsz on 4 a100 gives 75% accuracy\n",
    " default_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac82d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23,260,004 726-0.84 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_flops import get_flops\n",
    "# flops = get_flops(model, batch_size=1)\n",
    "\n",
    "# print(\"=\"*20)\n",
    "# print(f\"FLOPS: {flops / 10 ** 9:.03} G\")\n",
    "# print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d77584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "demo=glob.glob(\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/batch_28/**/**.png\")\n",
    "from PIL import Image\n",
    "from IPython.display import Image as Img\n",
    "\n",
    "img = Image.open(demo[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60138298",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_per_gpu=8\n",
    "image_shapes=(400,1400)\n",
    "batch_size=batch_per_gpu*devices\n",
    "input_shape=(None,image_shapes[0],image_shapes[1],3)\n",
    "NUM_EPOCHS=1\n",
    "learning_rate=2e-5\n",
    "classes=4\n",
    "zip_direc=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches\"\n",
    "opt=LAMB(learning_rate)\n",
    "\n",
    "#if any batch not present then it will be skipped\n",
    "names=['batch_28.zip','batch_18.zip','batch_108.zip',\n",
    "     'batch_106.zip','batch_85.zip','batch_123.zip',\n",
    "     'batch_7.zip','batch_48.zip','batch_2.zip','batch_114.zip',\n",
    "     'batch_125.zip','batch_67.zip','batch_4.zip']\n",
    "\n",
    "rescaling_layer=tf.keras.layers.Rescaling(scale=1./255) #we will use this later\n",
    "\n",
    "class resnet50: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None,pooling=None):\n",
    "        \n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.resnet50.ResNet50(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,pooling=pooling)\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        if(pooling==None):\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "#loading small validation data incase we are not abe to upload full validation data\n",
    "\n",
    "# sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/sub_sample_data\",\n",
    "#     image_size=image_shapes,\n",
    "#     batch_size=batch_size,\n",
    "#     seed=2,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "# with strategy.scope():\n",
    "#     model = resnet50.build(input_shape=input_shape,\n",
    "#                                classes=classes) \n",
    "\n",
    "\n",
    "#     #loss is sparse because we have int features\n",
    "#     model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "#     model.build(input_shape=input_shape)\n",
    "\n",
    "#     print(model.summary())\n",
    "\n",
    "\n",
    "# from keras_flops import get_flops\n",
    "# flops = get_flops(model, batch_size=1)\n",
    "\n",
    "# print(\"=\"*20)\n",
    "# print(f\"FLOPS: {flops / 10 ** 9:.03} G\")\n",
    "# print(\"=\"*20)\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"Resnet50-Imagenet\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "#print(config)\n",
    "    \n",
    "clf=Image_classifier_on_zipped_data(zip_direc,names)\n",
    "#clf.fit(model,log_file=\"Resnet.txt\",validation_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d72de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaling_layer=tf.keras.layers.Rescaling(scale=1./255) #we will use this later\n",
    "\n",
    "class densenet169: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None,pooling=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.densenet.DenseNet169(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,pooling=pooling)\n",
    "        \n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        if(pooling==None):\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "#loading small validation data incase we are not abe to upload full validation data\n",
    "\n",
    "# sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/sub_sample_data\",\n",
    "#     image_size=image_shapes,\n",
    "#     batch_size=batch_size,\n",
    "#     seed=2,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "with strategy.scope():\n",
    "    model = densenet169.build(input_shape=input_shape,\n",
    "                               classes=classes) \n",
    "\n",
    "\n",
    "    #loss is sparse because we have int features\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.build(input_shape=input_shape)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"densenet169-Imagenet\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "#print(config)\n",
    "    \n",
    "clf=Image_classifier_on_zipped_data(zip_direc,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaling_layer=tf.keras.layers.Rescaling(scale=1./255) #we will use this later\n",
    "\n",
    "class efficientnetB0: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None,pooling=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling=pooling)\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        if(pooling==None):\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "#loading small validation data incase we are not abe to upload full validation data\n",
    "\n",
    "# sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/sub_sample_data\",\n",
    "#     image_size=image_shapes,\n",
    "#     batch_size=batch_size,\n",
    "#     seed=2,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "with strategy.scope():\n",
    "    model = efficientnetB0.build(input_shape=input_shape,\n",
    "                               classes=classes) \n",
    "\n",
    "\n",
    "    #loss is sparse because we have int features\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.build(input_shape=input_shape)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"densenet169-Imagenet\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "#print(config)\n",
    "    \n",
    "clf=Image_classifier_on_zipped_data(zip_direc,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 batch per gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d60ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaling_layer=tf.keras.layers.Rescaling(scale=1./255) #we will use this later\n",
    "\n",
    "class Convnext: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None,pooling=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.convnext.ConvNeXtSmall(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling=pooling)\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        if(pooling==None):\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "#loading small validation data incase we are not abe to upload full validation data\n",
    "\n",
    "# sub_sample_validation_dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=\"/gpfsscratch/rech/zpf/uyf36me/vision_patches/sub_sample_data\",\n",
    "#     image_size=image_shapes,\n",
    "#     batch_size=batch_size,\n",
    "#     seed=2,\n",
    "#     shuffle=True\n",
    "#     )\n",
    "\n",
    "# sub_sample_validation_dataset=sub_sample_validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "with strategy.scope():\n",
    "    model = Convnext.build(input_shape=input_shape,\n",
    "                               classes=classes) \n",
    "\n",
    "\n",
    "    #loss is sparse because we have int features\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    model.build(input_shape=input_shape)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"densenet169-Imagenet\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "#print(config)\n",
    "    \n",
    "clf=Image_classifier_on_zipped_data(zip_direc,names)\n",
    "#clf.fit(model,log_file=\"Resnet.txt\",validation_data=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-geology",
   "metadata": {},
   "source": [
    "# Avoiding the gradient vanishing problem with skip connections (ResNets)\n",
    "\n",
    "Resnets (2015): https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "**Proposed 3 variants:**\n",
    "\n",
    "1. ResNet-50\n",
    "2. ResNet-101\n",
    "3. ResNet-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51cd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://media.geeksforgeeks.org/wp-content/uploads/20200424011138/ResNet.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet50: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.resnet50.ResNet50(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling='avg')\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "#adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#ideally we should double the epochs when drop out is 0.5\n",
    "NUM_EPOCHS = 30 \n",
    "\n",
    "model = resnet50.build(input_shape=input_shape,\n",
    "                           classes=classes) \n",
    "\n",
    "\n",
    "#loss is sparse because we have int features\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"Resnet50-Imagenet\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "print(config)\n",
    "\n",
    "\n",
    "wandb.login(key=\"d3c2d30bb5cdd697f78c2ab583f9bde909c7c7a8\")\n",
    "run=wandb.init(project=\"vision_approaches\",\n",
    "               config=config\n",
    "              )\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "\n",
    "#early stopping to see the training \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', min_delta=0.01,patience = 3)\n",
    "\n",
    "##checkpoint callback\n",
    "checkpoint_filepath=\"./checkpoint/\"+config[\"model_name\"]+\"/\"+\"my_model.h5\"\n",
    "\n",
    "print(checkpoint_filepath)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "### tensorboard callback\n",
    "tensorboard_cb=tf.keras.callbacks.TensorBoard(log_dir=\"my_log_dir\",histogram_freq=1)\n",
    "\n",
    "#wandb callback\n",
    "wb_cb=WandbCallback()\n",
    "\n",
    "print(\"Training network...\")\n",
    "\n",
    "#approimately 200h per epoch\n",
    "history = model.fit(train_dataset, validation_data=validation_dataset,epochs=NUM_EPOCHS,verbose=1,callbacks=[early_stopping_cb,model_checkpoint_cb,wb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one thing we can notice here is that the FLOPS and parameters of models have significantly increased\n",
    "#2016-2018 the focus was to build even deeper networks with least amount of FLOPS while also increasing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Densenets 2017\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/1400/1*rmHdoPjGUjRek6ozH7altw.png\")\n",
    "\n",
    "#everything is connected to everything from it's past "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1400/1*k75kOqrISzAEgtS15WWSZA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Densenet variants variants 121/169/201\n",
    "\n",
    "class densenet169: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.densenet.DenseNet169(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling='avg')\n",
    "        #print(base_model.summary())\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "#adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#ideally we should double the epochs when drop out is 0.5\n",
    "NUM_EPOCHS = 30 \n",
    "\n",
    "model = densenet169.build(input_shape=input_shape,\n",
    "                           classes=classes) \n",
    "\n",
    "\n",
    "#loss is sparse because we have int features\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"densenet169\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "print(config)\n",
    "\n",
    "\n",
    "wandb.login(key=\"d3c2d30bb5cdd697f78c2ab583f9bde909c7c7a8\")\n",
    "run=wandb.init(project=\"vision_approaches\",\n",
    "               config=config\n",
    "              )\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "\n",
    "#early stopping to see the training \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', min_delta=0.01,patience = 3)\n",
    "\n",
    "##checkpoint callback\n",
    "checkpoint_filepath=\"./checkpoint/\"+config[\"model_name\"]+\"/\"+\"my_model.h5\"\n",
    "\n",
    "print(checkpoint_filepath)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "### tensorboard callback\n",
    "tensorboard_cb=tf.keras.callbacks.TensorBoard(log_dir=\"my_log_dir\",histogram_freq=1)\n",
    "\n",
    "#wandb callback\n",
    "wb_cb=WandbCallback()\n",
    "\n",
    "print(\"Training network...\")\n",
    "\n",
    "#approimately 500h per epoch\n",
    "history = model.fit(train_dataset, validation_data=validation_dataset,epochs=NUM_EPOCHS,verbose=1,callbacks=[early_stopping_cb,model_checkpoint_cb,wb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NASnet neural arch search to find the right architecture on Cifar data that also works on Imagenet\n",
    "#the idea is to get a general machine searched architecture\n",
    "#policy gradient \n",
    "#28% lower FLOPS means faster to train\n",
    "#tensorflow comes with 2 variants Mobile/Large\n",
    "#we can try the large model should not be too bad in terms of computation\n",
    "\n",
    "# Densenet variants variants 121/169/201\n",
    "\n",
    "class Nasnet: \n",
    "    @staticmethod\n",
    "    def build(input_shape, classes,weights=None):\n",
    "        new_input_shape=(input_shape[1],input_shape[2],input_shape[3])\n",
    "        base_model=tf.keras.applications.nasnet.NASNetLarge(\n",
    "                                        include_top=False,\n",
    "                                        weights=weights,\n",
    "                                        input_shape=new_input_shape,\n",
    "                                        pooling='avg')\n",
    "        \n",
    "        print(len(base_model.layers))\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(rescaling_layer)\n",
    "        model.add(base_model)\n",
    "        # softmax classifier\n",
    "        model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "#adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#ideally we should double the epochs when drop out is 0.5\n",
    "NUM_EPOCHS = 30 \n",
    "\n",
    "model = Nasnet.build(input_shape=input_shape,\n",
    "                           classes=classes) \n",
    "\n",
    "\n",
    "#loss is sparse because we have int features\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.build(input_shape=input_shape)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "layers_depth_len=len(model.layers)\n",
    "total_params=(model.count_params())\n",
    "\n",
    "config=dict(num_epochs=NUM_EPOCHS,\n",
    "           model_name=\"NasnetLarge\",\n",
    "           total_params=total_params,\n",
    "           layers_depth_len=layers_depth_len)\n",
    "\n",
    "print(config)\n",
    "\n",
    "\n",
    "wandb.login(key=\"d3c2d30bb5cdd697f78c2ab583f9bde909c7c7a8\")\n",
    "run=wandb.init(project=\"vision_approaches\",\n",
    "               config=config\n",
    "              )\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "\n",
    "#early stopping to see the training \n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', min_delta=0.01,patience = 3)\n",
    "\n",
    "##checkpoint callback\n",
    "checkpoint_filepath=\"./checkpoint/\"+config[\"model_name\"]+\"/\"+\"my_model.h5\"\n",
    "\n",
    "print(checkpoint_filepath)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "### tensorboard callback\n",
    "tensorboard_cb=tf.keras.callbacks.TensorBoard(log_dir=\"my_log_dir\",histogram_freq=1)\n",
    "\n",
    "#wandb callback\n",
    "wb_cb=WandbCallback()\n",
    "\n",
    "print(\"Training network...\")\n",
    "\n",
    "#approimately  \n",
    "history = model.fit(train_dataset, validation_data=validation_dataset,epochs=NUM_EPOCHS,verbose=1,callbacks=[early_stopping_cb,model_checkpoint_cb,wb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-secretariat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu-2.11.0_py3.10.8",
   "language": "python",
   "name": "module-conda-env-tensorflow-gpu-2.11.0_py3.10.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
